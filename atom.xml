<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jeromezjl.github.io</id>
    <title>Jerome</title>
    <updated>2024-03-22T10:19:37.689Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jeromezjl.github.io"/>
    <link rel="self" href="https://jeromezjl.github.io/atom.xml"/>
    <subtitle>Jerome&apos;s blog</subtitle>
    <logo>https://jeromezjl.github.io/images/avatar.png</logo>
    <icon>https://jeromezjl.github.io/favicon.ico</icon>
    <rights>All rights reserved 2024, Jerome</rights>
    <entry>
        <title type="html"><![CDATA[【DL】图神经网络]]></title>
        <id>https://jeromezjl.github.io/post/dl-tu-shen-jing-wang-luo/</id>
        <link href="https://jeromezjl.github.io/post/dl-tu-shen-jing-wang-luo/">
        </link>
        <updated>2024-03-22T09:53:55.000Z</updated>
        <content type="html"><![CDATA[<p>图神经网络（Graph Neural Networks，GNNs）是一类专门处理图结构数据的神经网络。在图结构数据中，数据以图的形式表示，由节点（nodes）和边（edges）组成，非常适合描述物体（或实体）及其间的复杂关系。图神经网络通过直接在图上进行操作，能够有效地捕捉这种结构信息，因此在社交网络分析、推荐系统、蛋白质结构预测、化学分子建模等领域得到了广泛应用。</p>
<h3 id="核心思想">核心思想</h3>
<p>GNN的核心思想是节点表示的更新，这通过聚合来自邻居节点的信息来实现。每个节点的表示都是通过考虑其邻居节点的特征（以及可能的边的特征）进行更新的，这个过程可以迭代进行，直到达到一个稳定状态或者预定的迭代次数。这种信息的聚合方式使得每个节点能够捕捉到其在图中的局部结构信息。</p>
<h3 id="关键组件">关键组件</h3>
<ol>
<li><strong>节点表示</strong>：GNN的起点是节点的特征表示，这可以是节点的初始属性，也可以是节点的嵌入向量。</li>
<li><strong>聚合函数</strong>：用于聚合邻居节点信息的函数，这一步是GNN的核心。不同的GNN变体（如GCN、GAT等）在聚合函数的选择和设计上有所不同。</li>
<li><strong>更新函数</strong>：用于更新节点表示的函数。在每一次迭代中，节点的表示都会根据聚合的邻居信息进行更新。</li>
</ol>
<h3 id="gnn的变体">GNN的变体</h3>
<p>GNN有多种变体，主要包括：</p>
<ul>
<li><strong>图卷积网络（Graph Convolutional Networks, GCNs）</strong>：通过将卷积的概念推广到图上，对节点的特征进行聚合，从而更新节点的状态。</li>
<li><strong>图注意力网络（Graph Attention Networks, GATs）</strong>：引入了注意力机制来动态地确定在聚合邻居节点信息时每个邻居的重要性。</li>
<li><strong>图自编码器（Graph Autoencoders, GAEs）</strong>：用于学习图数据的低维表示，通常用于无监督学习任务。</li>
<li><strong>图生成网络（Graph Generative Networks, GGNs）</strong>：能够生成新的图结构数据，用于药物设计、蛋白质设计等领域。</li>
</ul>
<h3 id="应用">应用</h3>
<p>GNN在多个领域都有广泛应用，例如：</p>
<ul>
<li><strong>社交网络分析</strong>：通过分析社交网络中个体的关系图，进行好友推荐、信息传播分析等。</li>
<li><strong>推荐系统</strong>：利用用户和物品之间的复杂关系进行更精准的推荐。</li>
<li><strong>生物信息学</strong>：在蛋白质结构预测、基因表达数据分析等领域有着重要应用。</li>
<li><strong>化学和材料科学</strong>：用于预测分子的性质、设计新的化合物等。</li>
</ul>
<p>GNN由于能够直接在图结构数据上操作，捕捉复杂的关系和依赖，因此在处理此类数据时比传统的神经网络模型有着明显的优势。随着研究的深入和技术的发展，GNN在更多领域的应用也在不断拓展。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI 面试]]></title>
        <id>https://jeromezjl.github.io/post/ai-mian-shi/</id>
        <link href="https://jeromezjl.github.io/post/ai-mian-shi/">
        </link>
        <updated>2024-03-21T06:09:17.000Z</updated>
        <content type="html"><![CDATA[<ol>
<li>
<p>请简要解释分类和回归的区别。<br>
分类输出离散的类别，回归输出连续的值</p>
</li>
<li>
<p>您能列举一些常用的分类算法吗？请简要说明它们的工作原理。<br>
逻辑回归、决策树、随机森林、SVM、KNN、朴素贝叶斯、梯度提升决策树 (Gradient Boosted Decision Trees, GBDT)、CNN等</p>
</li>
<li>
<p>同样地，您能列举一些常用的回归算法吗？请简要说明它们的工作原理。<br>
线性回归、岭回归、SVM、KNN、SGD、贝叶斯、高斯过程</p>
</li>
<li>
<p>既可以用于分类又可以回归的算法？<br>
决策树、随机森林、SVM、KNN、梯度提升决策树 (Gradient Boosted Decision Trees, GBDT)、CNN等</p>
</li>
<li>
<p>在分类问题中，如何评估模型的性能？您能解释准确率、召回率、F1分数等指标吗？</p>
</li>
<li>
<p>在回归问题中，如何评估模型的性能？您能解释均方误差、均方根误差、决定系数等指标吗？</p>
</li>
<li>
<p>过拟合和欠拟合是什么？如何避免这两种情况？</p>
</li>
<li>
<p>在分类或回归任务中，如何处理不平衡数据集？<br>
处理不平衡数据集是机器学习中的一个常见问题，特别是在分类任务中。不平衡数据集指的是数据集中不同类别的样本数量差异很大。这可能会导致模型对多数类别过拟合，而忽略少数类别。以下是一些常用的方法来处理不平衡数据集：</p>
</li>
</ol>
<h3 id="1-重新采样技术">1. <strong>重新采样技术</strong>:</h3>
<ul>
<li><strong>过采样少数类</strong>: 通过复制少数类样本或通过生成类似样本的方法（如SMOTE - 合成少数过采样技术）来增加少数类样本的数量。</li>
<li><strong>欠采样多数类</strong>: 减少多数类样本的数量，以使多数类和少数类的样本数量大致相同。这可能导致信息丢失，应谨慎使用。</li>
</ul>
<h3 id="2-修改损失函数">2. <strong>修改损失函数</strong>:</h3>
<ul>
<li>为不同类别的样本引入不同的权重，使得模型在训练过程中更加关注少数类。这可以通过修改损失函数来实现，使得少数类样本的错误分类的代价更高。</li>
</ul>
<h3 id="3-使用集成方法">3. <strong>使用集成方法</strong>:</h3>
<ul>
<li>使用如随机森林、梯度提升树（GBT）等集成学习方法可以部分缓解不平衡数据带来的问题，因为它们通过构建多个模型并结合它们的预测结果来提高性能。</li>
</ul>
<h3 id="4-选择合适的评估指标">4. <strong>选择合适的评估指标</strong>:</h3>
<ul>
<li>在不平衡数据集上，准确度不再是一个好的性能指标。应该使用混淆矩阵、精确度、召回率、F1分数、ROC-AUC曲线等更复杂的指标来评估模型性能。</li>
</ul>
<h3 id="5-人工合成数据生成">5. <strong>人工合成数据生成</strong>:</h3>
<ul>
<li>使用算法如SMOTE（合成少数过采样技术）或ADASYN（自适应合成采样方法）等来合成新的少数类样本，以解决不平衡问题。</li>
</ul>
<h3 id="6-使用专门针对不平衡数据设计的算法">6. <strong>使用专门针对不平衡数据设计的算法</strong>:</h3>
<ul>
<li>一些算法如代价敏感学习（Cost-sensitive Learning）和异常检测算法在设计时考虑了不平衡数据的特点，可以直接应用于这类问题。</li>
</ul>
<h3 id="7-数据层面的策略">7. <strong>数据层面的策略</strong>:</h3>
<ul>
<li>收集更多数据，尤其是少数类的数据，有助于减少数据不平衡的问题，虽然这在实际中并不总是可行的。</li>
</ul>
<p>选择哪种方法取决于具体问题、数据集的特性以及可用资源。在实践中，经常需要尝试多种方法，并结合问题的具体情况来确定最有效的策略。</p>
<ol start="5">
<li>请解释特征选择和特征工程的重要性。</li>
<li>在机器学习中，如何处理缺失数据和异常值？<br>
请您根据自己的理解和经验回答这些问题，谢谢！</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[【NLP】Seq2Seq | Beam Search]]></title>
        <id>https://jeromezjl.github.io/post/nlp-seq2seq-beamsearch/</id>
        <link href="https://jeromezjl.github.io/post/nlp-seq2seq-beamsearch/">
        </link>
        <updated>2024-03-19T12:15:20.000Z</updated>
        <content type="html"><![CDATA[<h1 id="seq2seq">Seq2Seq</h1>
<p><a href="https://www.bilibili.com/video/BV16g411L7FG/?spm_id_from=333.999.0.0&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">62 序列到序列学习（seq2seq）【动手学深度学习v2】</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/194308943">Seq2Seq模型介绍</a><br>
Transformer：Seq2Seq model with attention</p>
<p>encoder-decoder 架构，使用的都是 RNN</p>
<h1 id="beam-search-束搜索">Beam Search 束搜索</h1>
<p>在选择softmax输出时，使用贪心算法（每次选择概率最大值）不一定能达到最优<br>
每次搜索保存k个最好的候选。k=1 是贪心，k=n 是穷举</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[【CV】概述]]></title>
        <id>https://jeromezjl.github.io/post/cv-gai-shu/</id>
        <link href="https://jeromezjl.github.io/post/cv-gai-shu/">
        </link>
        <updated>2024-03-15T11:55:57.000Z</updated>
        <content type="html"><![CDATA[<p>机器视觉算法是计算机视觉领域的关键组成部分，它使计算机能够通过图像和视频数据理解世界。这些算法可以根据它们的功能和用途进行分类。以下是一些常见的分类方式：</p>
<h3 id="1-图像处理算法">1. 图像处理算法</h3>
<ul>
<li><strong>预处理</strong>：包括去噪、对比度增强、颜色空间转换等。</li>
<li><strong>滤波和锐化</strong>：用于改善图像质量或提取特定特征。</li>
<li><strong>边缘检测</strong>：如Sobel、Canny算法，用于检测图像中的边缘。</li>
</ul>
<h3 id="2-特征提取算法">2. 特征提取算法</h3>
<ul>
<li><strong>角点检测</strong>：如Harris角点检测、Shi-Tomasi算法。</li>
<li><strong>兴趣点检测</strong>：如SIFT（尺度不变特征变换）、SURF（加速稳健特征）。</li>
<li><strong>特征描述子</strong>：如ORB（Oriented FAST and Rotated BRIEF）。</li>
</ul>
<h3 id="3-图像分类-image-classification">3. 图像分类 (Image Classification)</h3>
<ul>
<li><strong>任务说明</strong>：将整个图像分配给一个或多个类别。</li>
<li><strong>常用算法</strong>：
<ul>
<li>AlexNet</li>
<li>VGGNet</li>
<li>ResNet</li>
<li>Inception</li>
<li>DenseNet</li>
<li>EfficientNet</li>
<li>转移学习：使用预训练的CNN模型进行微调以适应特定的图像分类任务。</li>
</ul>
</li>
</ul>
<h3 id="4-目标检测与识别-object-detection">4. 目标检测与识别 (Object Detection)</h3>
<ul>
<li><strong>任务说明</strong>：在图像中识别物体的位置，并将每个物体分类。</li>
<li><strong>常用算法</strong>：
<ul>
<li>R-CNN及其变体（Fast R-CNN, Faster R-CNN）</li>
<li>YOLO系列（YOLOv1至YOLOv5）</li>
<li>SSD (Single Shot MultiBox Detector)</li>
<li>RetinaNet</li>
</ul>
</li>
<li><strong>目标识别</strong>：CNN 变体</li>
</ul>
<h3 id="5-图像分割-image-segmentation">5. 图像分割 (Image Segmentation)</h3>
<ul>
<li><strong>任务说明</strong>：将图像分割成多个区域或对象，可以进一步细分为语义分割和实例分割。</li>
<li><strong>常用算法</strong>：
<ul>
<li>FCN (Fully Convolutional Networks)</li>
<li>U-Net</li>
<li>Mask R-CNN（实例分割）</li>
<li>DeepLab系列</li>
<li>PSPNet (Pyramid Scene Parsing Network)</li>
</ul>
</li>
</ul>
<h3 id="6-姿态估计-pose-estimation">6. 姿态估计 (Pose Estimation)</h3>
<ul>
<li><strong>任务说明</strong>：估计图像中人或对象的姿态或关节位置。</li>
<li><strong>常用算法</strong>：
<ul>
<li>OpenPose</li>
<li>AlphaPose</li>
<li>DensePose</li>
<li>PoseNet</li>
</ul>
</li>
</ul>
<h3 id="7-物体跟踪-object-tracking">7. 物体跟踪 (Object Tracking)</h3>
<ul>
<li><strong>任务说明</strong>：在视频序列中跟踪一个或多个对象的运动。</li>
<li><strong>常用算法</strong>：
<ul>
<li>SiamFC (Siamese Fully Convolutional Network)</li>
<li>SORT/Simple Online and Realtime Tracking</li>
</ul>
</li>
</ul>
<h3 id="8-图像生成-image-generation">8. 图像生成 (Image Generation)</h3>
<ul>
<li><strong>任务说明</strong>：从现有的图像或随机噪声生成新图像。</li>
<li><strong>常用算法</strong>：
<ul>
<li>GANs (Generative Adversarial Networks) 及其变体（CGAN, DCGAN, StyleGAN）</li>
<li>VAEs (Variational Autoencoders)</li>
<li>PixelRNN/PixelCNN</li>
</ul>
</li>
</ul>
<h3 id="9-图像恢复-image-restoration">9. 图像恢复 (Image Restoration)</h3>
<ul>
<li><strong>任务说明</strong>：从损坏或降质的图像中恢复出清晰图像。</li>
<li><strong>常用算法</strong>：
<ul>
<li>SRCNN (Super-Resolution Convolutional Neural Network)</li>
<li>VDSR (Very Deep Super-Resolution)</li>
<li>GANs在图像超分辨率方面的应用</li>
<li>Denoising Autoencoders</li>
</ul>
</li>
</ul>
<h3 id="10-3d重建-3d-reconstruction">10. 3D重建 (3D Reconstruction)</h3>
<ul>
<li><strong>任务说明</strong>：从一系列图像中重建出三维场景或对象的结构。</li>
<li><strong>常用算法</strong>：
<ul>
<li>Multi-View Stereo (MVS)</li>
<li>COLMAP</li>
</ul>
</li>
</ul>
<h3 id="11-行为分析-action-analysis">11. 行为分析 (Action Analysis)</h3>
<ul>
<li><strong>任务说明</strong>：分析视频中的人或物体的行为，比如行人的行走路线、人群的动态等。</li>
<li><strong>常用算法</strong>：
<ul>
<li>C3D (Convolutional 3D Networks)</li>
<li>I3D (Inflated 3D ConvNet)</li>
</ul>
</li>
</ul>
<h3 id="12-视觉问答-visual-question-answering">12. 视觉问答 (Visual Question Answering)</h3>
<ul>
<li><strong>任务说明</strong>：根据给定图像和自然语言问题提供答案。</li>
<li><strong>常用算法</strong>：
<ul>
<li>基于注意力机制的模型</li>
<li>LSTM (Long Short-Term Memory) 网络</li>
<li>End-to-End模型</li>
<li>Transformer模型及其在视觉问答中的应用</li>
</ul>
</li>
</ul>
<h3 id="13-优化和机器学习算法">13. 优化和机器学习算法</h3>
<pre><code>- 用于提高识别准确率和效率的算法，如支持向量机（SVM）、随机森林、梯度提升决策树（GBDT）等。
- 
</code></pre>
<p>这些任务和算法展示了机器视觉领域的广泛性和深度，随着研究的进展，还会不断有新</p>
<p>的任务和算法被提出。</p>
<h1 id="数据增强">数据增强</h1>
<p>对图片进行翻转、裁剪、变色（颜色、亮度、饱和度）等操作<br>
<a href="https://www.bilibili.com/video/BV17y4y1g76q/?spm_id_from=333.999.0.0&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">36 数据增广【动手学深度学习v2】</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[【NLP】文本预处理]]></title>
        <id>https://jeromezjl.github.io/post/nlp-wen-ben-yu-chu-li/</id>
        <link href="https://jeromezjl.github.io/post/nlp-wen-ben-yu-chu-li/">
        </link>
        <updated>2024-03-15T03:49:11.000Z</updated>
        <content type="html"><![CDATA[<p>数据预处理是自然语言处理（NLP）任务中至关重要的一步，它直接影响到模型训练的效果和最终结果的质量。预处理步骤的目的是将原始文本转换成一种更易于计算机理解和处理的格式。以下是数据预处理中常见的几个步骤：</p>
<ol>
<li>
<p><strong>文本清洗</strong>:<br>
移除无关内容（如HTML标签）、标点符号、数字等，或者将它们转换成有意义的代替文本。</p>
<ul>
<li><strong>去除噪声</strong>：移除文本中的无关信息，如HTML标签、非文本内容（图片链接、JavaScript代码等）、格式符号等。</li>
<li><strong>规范化文本</strong>：将文本统一为标准格式，例如，将所有字母转换为小写（或大写），以减少词汇的变体数量。</li>
<li><strong>去除特殊字符和标点</strong>：根据任务需求，移除或替换文本中的特殊字符和标点符号，有时标点可以保留，因为它可能含有语义信息。</li>
</ul>
</li>
<li>
<p><strong>分词 (Tokenization)</strong>:</p>
<ul>
<li><strong>词级分词</strong>：将句子分割成单词或词汇单元，如短语。这是英文等西方语言中最常见的分词方式。</li>
<li><strong>子词级分词</strong>：将单词进一步分割成更小的单位（如词根、词缀）。这对于处理一些复合词或未见过的词特别有用。</li>
<li><strong>字符级分词</strong>：将文本分割成字符。这种方法对于某些任务或语言（如中文）可能更合适。</li>
</ul>
</li>
<li>
<p><strong>停用词去除</strong>:</p>
<ul>
<li>停用词是指在文本中频繁出现但对于理解文本意义贡献不大的词，如“的”、“是”、“在”等。去除这些词可以帮助减少数据噪声和特征维度。</li>
</ul>
</li>
<li>
<p><strong>词干提取 (Stemming) 和词形还原 (Lemmatization)</strong>:</p>
<ul>
<li><strong>词干提取</strong>：通过去除词缀来将词汇还原到基本形式（可能不是真正的词）。例如，“running”、“runs”词干提取后都变为“run”。</li>
<li><strong>词形还原</strong>：将词汇还原到其词典形式（lemma），考虑了词汇的词性。比如，“better”的词形还原结果是“good”。</li>
</ul>
</li>
<li>
<p><strong>数据增强</strong>:</p>
<ul>
<li>通过词替换、句子重排等方法人为增加训练数据的多样性，有助于改善模型的泛化能力。</li>
</ul>
</li>
<li>
<p><strong>向量化 (Vectorization)</strong>:</p>
<ul>
<li><strong>构建词汇表</strong>：统计词频，小于某个词频的词将不会被加入词汇表</li>
<li><strong>Token 编码</strong>：在构建了词汇表之后，每个唯一的token都会被分配一个唯一的数字ID。向量化的这一阶段涉及将文本中的每个token替换成对应的数字ID。这个过程实际上是一种编码，将文本数据转换为模型可以处理的数值形式。</li>
<li><strong>句子/文本向量化（tokenize）</strong>：完成token的数字编码后，整个句子或文本片段可以表示为一个数字序列。</li>
<li><strong>序列填充 (Padding) 和截断</strong>：对于需要固定长度输入的模型（如很多深度学习模型），需要通过填充（通常用0或特殊标记<code>&lt;PAD&gt;</code> ）或截断来使所有文本序列长度一致。</li>
<li><strong>词袋模型 (Bag of Words, BoW)</strong>：将文本转换为词频向量，但这种方法不考虑词序和上下文。</li>
<li><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>：一种加权的词袋模型，考虑了词在文档集中的稀有程度。</li>
<li><strong>词嵌入 (Word Embeddings)</strong>：将词汇token映射到连续的向量空间中，这些向量捕获了词汇之间的语义关系。常见的词嵌入模型包括Word2Vec、GloVe和BERT等。</li>
</ul>
</li>
</ol>
<p>每个步骤的具体实现和必要性可能会根据具体的任务和语言而有所不同。例如，对于某些任务，保留标点符号可能是有意义的，因为它们可以携带情感或语法信息。而对于一些语言（如中文、日文），分词步骤会比英文复杂得多，可能需要特定的算法和词库。预处理的目的是清洗和转换数据，以提高模型训练的效率和效果。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[【NLP】概述]]></title>
        <id>https://jeromezjl.github.io/post/nlp-gai-shu/</id>
        <link href="https://jeromezjl.github.io/post/nlp-gai-shu/">
        </link>
        <updated>2024-03-15T03:19:09.000Z</updated>
        <content type="html"><![CDATA[<h1 id="任务">任务</h1>
<ol>
<li><strong>文本分类（Text Classification）</strong>：将文本数据分类到预定义的类别中。常见的应用包括垃圾邮件检测、情感分析和主题分类。</li>
</ol>
<ul>
<li>朴素贝叶斯（Naive Bayes）</li>
<li>支持向量机（SVM）</li>
<li>随机森林（Random Forest）</li>
<li>卷积神经网络（CNN）</li>
<li>长短期记忆网络（LSTM）</li>
<li>Transformer模型（如BERT）</li>
</ul>
<ol start="2">
<li><strong>命名实体识别（Named Entity Recognition, NER）</strong>：识别文本中的命名实体，如人名、地点名、组织名等，并将其分类到预定义的类别。</li>
</ol>
<ul>
<li>条件随机场（CRF）</li>
<li>循环神经网络（RNN）</li>
<li>长短期记忆网络（LSTM）+ CRF</li>
<li>BERT及其变体</li>
</ul>
<ol start="3">
<li><strong>词性标注（Part-of-Speech Tagging, POS Tagging）</strong>：为文本中的每个单词分配一个词性标签，如名词、动词、形容词等。</li>
</ol>
<ul>
<li>隐马尔可夫模型（HMM）</li>
<li>条件随机场（CRF）</li>
<li>循环神经网络（RNN）</li>
<li>Transformer模型（如BERT）</li>
</ul>
<ol start="4">
<li><strong>句法分析（Syntactic Parsing）</strong>：分析句子的语法结构，确定单词之间的依赖关系和句子的语法树结构。</li>
</ol>
<ul>
<li>基于规则的方法</li>
<li>上下文无关文法（CFG）</li>
<li>依存解析（Dependency Parsing）使用神经网络</li>
<li>Transformer模型（如BERT、GPT）</li>
</ul>
<ol start="5">
<li><strong>语义分析（Semantic Analysis）</strong>：理解句子或文本的含义，包括词义消歧和语义角色标注。</li>
</ol>
<ul>
<li>潜在语义分析（LSA）</li>
<li>潜在狄利克雷分配（LDA）</li>
<li>词嵌入方法（如Word2Vec、GloVe）</li>
<li>Transformer模型（如BERT、RoBERTa）</li>
</ul>
<ol start="6">
<li><strong>情感分析（Sentiment Analysis）</strong>：判断文本的情感倾向，如正面、负面或中性。</li>
</ol>
<ul>
<li>朴素贝叶斯（Naive Bayes）</li>
<li>支持向量机（SVM）</li>
<li>循环神经网络（RNN）</li>
<li>长短期记忆网络（LSTM）</li>
<li>Transformer模型（如BERT、XLNet）</li>
</ul>
<ol start="7">
<li><strong>文本摘要（Text Summarization）</strong>：生成文本的简短且含义完整的摘要。</li>
</ol>
<ul>
<li>抽取式摘要方法，如TF-IDF</li>
<li>序列到序列模型（Seq2Seq），如LSTM</li>
<li>注意力机制（Attention Mechanism）</li>
<li>预训练语言模型（如GPT-3、BERT）</li>
</ul>
<ol start="8">
<li><strong>机器翻译（Machine Translation）</strong>：将一种语言的文本翻译成另一种语言。</li>
</ol>
<ul>
<li>统计机器翻译（SMT）</li>
<li>序列到序列模型（Seq2Seq）</li>
<li>注意力机制</li>
<li>Transformer架构</li>
</ul>
<ol start="9">
<li><strong>问答系统（Question Answering）</strong>：对自然语言形式的问题给出直接答案。</li>
</ol>
<ul>
<li>信息检索技术</li>
<li>长短期记忆网络（LSTM）</li>
<li>注意力机制</li>
<li>BERT和Transformer模型</li>
</ul>
<ol start="10">
<li><strong>对话系统和聊天机器人（Dialogue Systems and Chatbots）</strong>：构建能够与人类用户进行自然对话的系统。</li>
</ol>
<ul>
<li>序列到序列模型（Seq2Seq）</li>
<li>循环神经网络（RNN）</li>
<li>长短期记忆网络（LSTM）</li>
<li>Transformer和GPT系列</li>
</ul>
<ol start="11">
<li><strong>文本生成（Text Generation）</strong>：基于某些输入生成自然语言文本，如新闻文章生成、故事创作等。</li>
</ol>
<ul>
<li>马尔可夫模型</li>
<li>循环神经网络（RNN）</li>
<li>长短期记忆网络（LSTM）</li>
<li>GPT系列</li>
</ul>
<ol start="12">
<li><strong>语音识别（Speech Recognition）</strong>：将语音信号转换为文本。</li>
</ol>
<ul>
<li>隐马尔可夫模型（HMM）</li>
<li>深度神经网络（DNN）</li>
<li>长短期记忆网络（LSTM）</li>
<li>端到端的深度学习模型</li>
</ul>
<ol start="13">
<li><strong>自然语言理解（Natural Language Understanding, NLU）</strong>：深入理解自然语言的含义和上下文。</li>
</ol>
<ul>
<li>词嵌入（Word Embeddings）</li>
<li>长短期记忆网络（LSTM）</li>
<li>Transformer模型</li>
<li>BERT及其变体</li>
</ul>
<ol start="14">
<li><strong>自然语言生成（Natural Language Generation, NLG）</strong>：从非语言数据生成人类可理解的语言。</li>
</ol>
<ul>
<li>模板方法</li>
<li>序列到序列模型（Seq2Seq）</li>
<li>Transformer模型</li>
<li>GPT系列</li>
</ul>
<ol start="15">
<li><strong>关键词提取（Keyword Extraction）</strong>：从文本中提取最相关的词汇或短语。</li>
</ol>
<ul>
<li>TF-IDF</li>
<li>TextRank</li>
<li>LDA（潜在狄利克雷分配）</li>
<li>BERT Embeddings</li>
</ul>
<ol start="16">
<li><strong>主题建模（Topic Modeling）</strong>：无监督地识别大量文档集中的潜在主题。</li>
</ol>
<ul>
<li>潜在语义分析（LSA）</li>
<li>潜在狄利克雷分配（LDA）</li>
<li>非负矩阵分解（NMF）</li>
</ul>
<p>对于每种任务，选择最合适的算法通常取决于具体的应用场景、可用数据的量和质以及性能要求。随着深度学习技术的发展，基于Transformer的模型如BERT、GPT系列在多个NLP任务中取得了突破性的成果。</p>
<h1 id="算法">算法</h1>
<p>自然语言处理（NLP）领域中有多种算法和技术，这些方法旨在帮助计算机理解、解释和生成人类语言。以下是一些核心的NLP算法和技术：</p>
<ol>
<li>
<p><strong>基于规则的系统</strong>：早期的NLP系统大多依赖于手写的规则来解析和理解文本。这些规则可以基于语法、句法和语义规则来设计。</p>
</li>
<li>
<p><strong>统计方法</strong>：</p>
<ul>
<li><strong>隐马尔可夫模型（HMM）</strong>：用于词性标注和命名实体识别等任务。</li>
<li><strong>条件随机场（CRF）</strong>：用于序列建模，如标注问题和命名实体识别。</li>
</ul>
</li>
<li>
<p><strong>机器学习算法</strong>：随着机器学习的发展，许多传统算法被用于NLP任务，如朴素贝叶斯、决策树、支持向量机（SVM）等。</p>
</li>
<li>
<p><strong>深度学习/神经网络方法</strong>：近年来，深度学习在NLP中取得了重大进展，以下是一些关键的神经网络架构：</p>
<ul>
<li><strong>卷积神经网络（CNNs）</strong>：虽然最初用于图像处理，但也被适用于处理文本数据，如句子分类任务。</li>
<li><strong>循环神经网络（RNNs）</strong>：特别适合处理序列数据，如时间序列或文本。长短期记忆网络（LSTMs）和门控循环单元（GRUs）是RNN的变体，能够解决传统RNNs的梯度消失问题。</li>
<li><strong>注意力机制和Transformer架构</strong>：注意力机制允许模型在处理序列数据时更加灵活地权衡不同部分的重要性，而Transformer架构则彻底改变了NLP领域，成为了多种任务的基础，如BERT、GPT系列、RoBERTa、T5等。</li>
</ul>
</li>
<li>
<p><strong>预训练语言模型</strong>：利用大量无标签文本数据进行预训练，然后在特定任务上进行微调。BERT和GPT系列是这一范式下的两个典型例子。</p>
</li>
<li>
<p><strong>迁移学习和微调</strong>：借助预训练的语言模型，通过在特定任务上的微调，可以显著提高性能。这种方法减少了对大量标记数据的依赖。</p>
</li>
</ol>
<p>这些算法和技术在各种NLP任务中被广泛应用，如文本分类、情感分析、机器翻译、语音识别、问答系统、文本摘要、自然语言生成等。随着研究的不断进展，新的算法和模型也在不断被提出和改进。</p>
<h1 id="nlp-任务的一般过程">NLP 任务的一般过程</h1>
<ol>
<li>
<p><strong>问题定义</strong>:</p>
<ul>
<li>明确任务目标：这可能是文本分类、情感分析、机器翻译、命名实体识别、问答系统等。</li>
<li>确定输入输出：定义任务的输入数据（如文本、句子、段落）和期望的输出（如类别标签、文本响应等）。</li>
</ul>
</li>
<li>
<p><strong>数据收集</strong>:</p>
<ul>
<li>收集足够的数据：根据任务需求，收集标注好的训练数据。对于一些任务，还可能需要收集未标注的数据进行无监督学习或半监督学习。</li>
<li>来源：数据可以来自公共数据集、网络爬虫、社交媒体、公司数据库等。</li>
</ul>
</li>
<li>
<p><strong>数据预处理</strong>:</p>
<ul>
<li>文本清洗：移除无关内容（如HTML标签）、标点符号、数字等，或者将它们转换成有意义的代替文本。</li>
<li>分词：将文本分割成单词、短语或其他有意义的单位。</li>
<li>规范化：包括小写转换、词干提取、词形还原等，旨在将单词规范到基本形式。</li>
<li>去除停用词：移除常见但对于理解文本意义不大的词，如“的”、“是”、“在”等。</li>
<li>向量化：将文本转换为数值形式，常见方法包括词袋模型、TF-IDF、词嵌入等。</li>
</ul>
</li>
<li>
<p><strong>特征工程</strong>:</p>
<ul>
<li>特征提取：根据任务需求，选择或设计文本特征，如n-gram、词频、词嵌入向量等。</li>
<li>降维：对高维特征空间应用降维技术，如PCA、t-SNE，以减少计算复杂度。</li>
</ul>
</li>
<li>
<p><strong>模型选择和训练</strong>:</p>
<ul>
<li>选择模型：根据任务类型选择合适的模型，可能是传统机器学习模型（如SVM、随机森林）或深度学习模型（如CNN、RNN、Transformer）。</li>
<li>训练模型：使用训练数据训练模型，调整超参数以获得最佳性能。</li>
</ul>
</li>
<li>
<p><strong>评估和优化</strong>:</p>
<ul>
<li>使用验证集评估模型性能，采用适当的评估指标（如准确率、召回率、F1分数、BLEU分数等）。</li>
<li>根据评估结果调整模型结构、超参数等，可能包括使用更复杂的模型、增加更多训练数据、应用不同的预处理或特征工程技术。</li>
</ul>
</li>
<li>
<p><strong>部署和监控</strong>:</p>
<ul>
<li>将训练好的模型部署到生产环境，使其能够处理实时数据或新数据。</li>
<li>监控模型性能，定期检查并重新训练模型以适应新数据或变化的数据分布。</li>
</ul>
</li>
<li>
<p><strong>反馈循环</strong>:</p>
<ul>
<li>根据模型在实际应用中的表现，收集反馈，可能需要重新执行前面的步骤，如重新定义问题、收集更多或更高质量的数据、重新训练模型等。</li>
</ul>
</li>
</ol>
<p>这个流程不是一成不变的，具体的步骤和方法可能会根据具体的NLP任务、数据集、业务需求等因素有所不同。</p>
<h1 id="评估指标">评估指标</h1>
<p><a href="https://blog.csdn.net/ph12345687/article/details/130205151">NLP常见任务及评估指标</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[【DL】强化学习]]></title>
        <id>https://jeromezjl.github.io/post/dl-qiang-hua-xue-xi/</id>
        <link href="https://jeromezjl.github.io/post/dl-qiang-hua-xue-xi/">
        </link>
        <updated>2024-03-14T14:40:15.000Z</updated>
        <content type="html"><![CDATA[<p>强化学习是机器学习的一个领域，它主要关注如何使智能体（Agent）在环境（Environment）中学会采取行动（Action）以最大化某种累积奖励（Reward）。强化学习与其他类型的机器学习（如监督学习和无监督学习）的主要区别在于，它不依赖于预先标记的输入/输出对，而是通过智能体与环境的交互来学习。以下是这些概念的详细介绍：</p>
<ol>
<li>
<p><strong>智能体（Agent）</strong>：</p>
<ul>
<li>智能体是强化学习中的决策者，它通过观察环境来学习如何采取行动。智能体的目标是通过这些行动来最大化其长期奖励。它可以是任何能够感知其环境并根据这些观察做出决策的实体，如机器人、软件程序等。</li>
</ul>
</li>
<li>
<p><strong>环境（Environment）</strong>：</p>
<ul>
<li>环境是智能体所处并与之互动的系统或问题域。环境接收智能体的行动并根据这些行动提供状态的反馈和奖励。环境的反馈可以是非常简单的，也可以是极其复杂且动态变化的。</li>
</ul>
</li>
<li>
<p><strong>状态（State）</strong>：</p>
<ul>
<li>状态是对环境在某一时刻的描述。它可以是环境的完整描述，也可以只是环境的一部分。状态为智能体提供了决策的上下文，在给定状态下采取行动可以导致环境状态的变化。</li>
</ul>
</li>
<li>
<p><strong>动作（Action）</strong>：</p>
<ul>
<li>动作是智能体在给定状态下可以采取的决策或步骤。智能体的动作空间可以是离散的（如左转、右转）或连续的（如加速的量）。智能体的行动会影响环境，并导致状态的变化和奖励的给予。</li>
</ul>
</li>
<li>
<p><strong>奖励（Reward）</strong>：</p>
<ul>
<li>奖励是环境对智能体采取特定行动的即时评价。奖励通常是一个标量值，指示智能体的行动对于达成其目标的有用程度。智能体的目标是最大化在整个学习过程中获得的累积奖励。</li>
</ul>
</li>
<li>
<p><strong>策略（Policy）</strong>：</p>
<ul>
<li>策略是从状态到动作的映射，它定义了智能体在给定状态下应该采取什么行动。策略可以是简单的静态规则，也可以是复杂的动态函数，取决于智能体的学习算法和环境的性质。智能体的目标是学习一个最优策略，这个策略能在长期内最大化累积奖励。</li>
</ul>
</li>
</ol>
<p>强化学习的过程过程可以通过多种算法来通常涉及智能体不断地通过与环境互动来尝试不同的策略，评估这些策略带来的奖励，并根据这些奖励来调整其策略，以学习如何最优地行动。这个实现，如Q学习、深度Q网络（DQN）、策略梯度方法等。</p>
<h1 id="常见算法">常见算法</h1>
<h2 id="动态规划">动态规划</h2>
<p>动态规划（Dynamic Programming, DP）在强化学习中扮演着基础而重要的角色，尤其是在处理具有完全已知的环境模型（即状态转移概率和奖励函数已知）的问题时。DP方法依赖于贝尔曼方程，这是一组递归方程，用于描述状态值函数或动作值函数（即Q函数）之间的关系。在强化学习中，动态规划法主要用于两个方面：策略评估（Policy Evaluation）和策略提升（Policy Improvement），这两个步骤循环交替执行以找到最优策略。</p>
<h3 id="策略评估policy-evaluation">策略评估（Policy Evaluation）</h3>
<p>策略评估的目标是计算某策略下的状态值函数，即在遵循特定策略的条件下，从某状态开始所能获得的预期回报。通过迭代地应用贝尔曼期望方程，我们可以评估当前策略的效果：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>V</mi><mi>π</mi></msup><mo>(</mo><mi>s</mi><mo>)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mi>π</mi><mo>(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo>)</mo><munder><mo>∑</mo><mrow><msup><mi>s</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><mi>r</mi></mrow></munder><mi>P</mi><mo>(</mo><msup><mi>s</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><mi>r</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo><mo>[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msup><mi>V</mi><mi>π</mi></msup><mo>(</mo><msup><mi>s</mi><mo mathvariant="normal">′</mo></msup><mo>)</mo><mo>]</mo></mrow><annotation encoding="application/x-tex">V^{\pi}(s) = \sum_{a \in A} \pi(a|s) \sum_{s&#x27;, r} P(s&#x27;, r | s, a)[r + \gamma V^{\pi}(s&#x27;)]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">π</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.480098em;vertical-align:-1.430093em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8556639999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="mrel mtight">∈</span><span class="mord mathdefault mtight">A</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.321706em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mord">∣</span><span class="mord mathdefault">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8560149999999997em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828285714285715em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.430093em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord">∣</span><span class="mord mathdefault">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mclose">)</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">π</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></p>
<p>其中，$$  V^{\pi}(s) $$ 是在状态 ( s ) 下遵循策略 ( \pi ) 所得到的价值，( \pi(a|s) ) 是在状态 ( s ) 下采取动作 ( a ) 的概率，( P(s', r | s, a) ) 是从状态 ( s ) 采取动作 ( a ) 转移到状态 ( s' ) 并得到奖励 ( r ) 的概率，( \gamma ) 是折扣因子，用于计算未来奖励的现值。</p>
<h3 id="策略提升policy-improvement">策略提升（Policy Improvement）</h3>
<p>策略提升的目标是生成一个新策略，该策略在每个状态下都可以选择使动作价值最大化的动作。通过这个过程，我们可以从当前策略产生一个更好的策略。策略提升通常使用贝尔曼最优方程：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>Q</mi><mi>π</mi></msup><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo><mo>=</mo><munder><mo>∑</mo><mrow><msup><mi>s</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><mi>r</mi></mrow></munder><mi>P</mi><mo>(</mo><msup><mi>s</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><mi>r</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo><mo>[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msup><mi>V</mi><mi>π</mi></msup><mo>(</mo><msup><mi>s</mi><mo mathvariant="normal">′</mo></msup><mo>)</mo><mo>]</mo></mrow><annotation encoding="application/x-tex">Q^{\pi}(s, a) = \sum_{s&#x27;, r} P(s&#x27;, r | s, a)[r + \gamma V^{\pi}(s&#x27;)]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">π</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.480098em;vertical-align:-1.430093em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8560149999999997em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828285714285715em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.430093em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord">∣</span><span class="mord mathdefault">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mclose">)</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">π</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></p>
<p>然后，新的策略 ( \pi' ) 在每个状态下选择最大化 ( Q^{\pi}(s, a) ) 的动作：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>π</mi><mo mathvariant="normal">′</mo></msup><mo>(</mo><mi>s</mi><mo>)</mo><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mi>max</mi><mo>⁡</mo><mi>a</mi></munder><msup><mi>Q</mi><mi>π</mi></msup><mo>(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\pi&#x27;(s) = \arg\max_a Q^{\pi}(s, a)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.45em;vertical-align:-0.7em;"></span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.43056em;"><span style="top:-2.1em;margin-left:0em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">π</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mclose">)</span></span></span></span></span></p>
<h3 id="策略迭代policy-iteration">策略迭代（Policy Iteration）</h3>
<p>策略迭代结合了策略评估和策略提升，通过迭代这两个步骤直到策略收敛到最优策略。每次迭代包括完全评估当前策略（直到值函数收敛），然后进行策略提升。</p>
<h3 id="值迭代value-iteration">值迭代（Value Iteration）</h3>
<p>值迭代是策略迭代的一种简化形式，它结合了策略评估的一步操作和策略提升。值迭代直接迭代更新每个状态的最大动作价值，直到价值函数收敛：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi><mo>(</mo><mi>s</mi><mo>)</mo><mo>=</mo><munder><mi>max</mi><mo>⁡</mo><mi>a</mi></munder><munder><mo>∑</mo><mrow><msup><mi>s</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><mi>r</mi></mrow></munder><mi>P</mi><mo>(</mo><msup><mi>s</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><mi>r</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo>)</mo><mo>[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><mi>V</mi><mo>(</mo><msup><mi>s</mi><mo mathvariant="normal">′</mo></msup><mo>)</mo><mo>]</mo></mrow><annotation encoding="application/x-tex">V(s) = \max_a \sum_{s&#x27;, r} P(s&#x27;, r | s, a)[r + \gamma V(s&#x27;)]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.480098em;vertical-align:-1.430093em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.43056em;"><span style="top:-2.1em;margin-left:0em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8560149999999997em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828285714285715em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.430093em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord">∣</span><span class="mord mathdefault">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mclose">)</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></p>
<p>动态规划方法在理论上可以得到最优策略，但其应用受限于环境模型的已知性以及状态和动作空间的规模。当状态和动作空间非常大或环境模型未知时，直接应用动态规划变得不可行，这时通常会</p>
<h2 id="马尔可夫">马尔可夫</h2>
<h2 id="蒙特卡洛">蒙特卡洛</h2>
<p>蒙特卡洛方法在强化学习中的应用提供了一种在不完全知道环境模型（即转移概率和奖励函数未知）的情况下学习最优策略的方法。与动态规划不同，蒙特卡洛（MC）方法不需要知道环境的具体动态，而是通过从环境中采样完成的序列（或称为“情节”）来学习。这些方法特别适用于具有高度不确定性或模型未知的环境。MC方法的关键特点是它们依赖于经验平均来估计值函数，这些平均值是从一系列完整情节中获得的。</p>
<h3 id="mc策略评估">MC策略评估</h3>
<p>在策略评估过程中，MC方法通过对从当前策略生成的一系列完整情节的回报进行平均来估计状态的值。每个情节包含一系列的状态、动作和奖励，以及情节的最终结果。通过对同一状态多次访问得到的回报进行平均，MC方法可以估计该状态的值，即该状态的长期回报期望。</p>
<h3 id="mc控制">MC控制</h3>
<p>为了找到最优策略，MC方法采用了一种称为MC控制的方法。MC控制通常涉及两个主要步骤：探索和利用。一个常见的策略是使用ε-贪婪策略，其中智能体大部分时间选择当前最佳动作（利用），但有时也随机选择其他动作（探索），以确保长期学习。</p>
<p>MC控制方法通过不断交替执行策略评估和策略提升来工作。在策略评估阶段，智能体使用其当前策略在环境中执行动作，并记录下来每个情节的结果。接着，使用这些情节的结果来更新值函数。在策略提升阶段，智能体根据估计的值函数更新其策略，通常是通过选择使得估计值函数最大化的动作。</p>
<h3 id="优点与局限">优点与局限</h3>
<p>MC方法的一个主要优点是它们不依赖于环境的先验知识，使其适用于模型未知的情况。此外，MC方法直接从最终回报中学习，而不依赖于其他状态的值估计，这有助于减少累积的估计误差。</p>
<p>然而，MC方法也有其局限性。首先，它们只适用于情节性任务，即那些有明确开始和结束的任务。其次，MC方法可能需要很多情节才能获得可靠的值估计，尤其是在回报信号稀疏或噪声很大的情境中。此外，MC方法只在情节结束时更新值函数和策略，这可能导致学习速度较慢。</p>
<p>总的来说，蒙特卡洛方法在强化学习中提供了一种强大的工具，尤其是在那些模型未知或难以精确建模的情况下。通过适当的策略和技巧（如重要性采样）的改进，可以进一步增强MC方法的应用范围和效率。</p>
<h2 id="时序差分法">时序差分法</h2>
<h3 id="q-learning">Q-Learning</h3>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[【NLP】时序模型与马尔可夫模型]]></title>
        <id>https://jeromezjl.github.io/post/nlp-shi-xu-mo-xing-yu-ma-er-ke-fu-mo-xing/</id>
        <link href="https://jeromezjl.github.io/post/nlp-shi-xu-mo-xing-yu-ma-er-ke-fu-mo-xing/">
        </link>
        <updated>2024-03-12T12:24:02.000Z</updated>
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/weixin_39653948/article/details/105571760">原理+论文+实战：60篇由浅入深的时间序列预测/分类教程汇总</a></p>
<h1 id="时序模型">时序模型</h1>
<p>t 时刻的状态和前面的数据相关<br>
自回归：预测（同一序列）后面的值，是根据（同一序列）前面的值来预测，即使用自身过去数据预测未来</p>
<h1 id="潜变量模型">潜变量模型</h1>
<p><img src="https://jeromezjl.github.io/post-images/1710309825352.jpg" alt="" loading="lazy"><br>
h' 包含了 h 和 x 的信息，也就是之前状态的信息</p>
<h1 id="马尔可夫模型">马尔可夫模型</h1>
<p><a href="https://www.bilibili.com/video/BV1Mr4y1f7Nm?p=1&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">火遍油管！终于有大神把【马尔可夫链】给做成动画了！</a></p>
<p>马尔可夫性质：系统的下一个状态只依赖于当前状态，而与之前的状态无关。具有“无记忆性”</p>
<p>马尔可夫假设：第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关。N-gram模型就是基于该假设构建的</p>
<h1 id="马尔科夫链">马尔科夫链</h1>
<p>是依赖于马尔可夫假设的最基本的模型，用于模拟随即系统的状态转移<br>
未来的状态只取决于现在的状态，和之前的步骤无关<br>
任何状态所有出箭头概率之和为1<br>
转移状态矩阵：类似于有向图<br>
要找到在 n 步转移中，由状态 i 转移到状态 j 的概率，只需要找到 n 阶状态转移矩阵第 i 行 j 列</p>
<h1 id="隐马尔可夫模型-hmm">隐马尔可夫模型 HMM</h1>
<p>HMM 在马尔可夫链的基础上增加了“隐状态”和“观测状态”的概念。HMM 在马尔可夫链的基础上加入了观测数据，用于处理观测与状态间存在某种隐含关系的情况。</p>
<p>在 HMM 中，系统的真实状态（隐状态）不能直接观测，只能通过一些观测到的事件（观测状态）间接推断。</p>
<p>HMM 被广泛应用于序列数据的建模，如语音识别、生物序列分析等，其中系统的内部状态不是直接可见的。</p>
<p>隐马尔可夫模型（Hidden Markov Model, HMM）是一种统计模型，它用于描述一个观测序列背后的一个不可观测（隐含）状态序列。这个模型假设隐含状态生成观测数据的过程具有马尔可夫性质，即每个状态的发生仅依赖于它之前的一个状态。隐马尔可夫模型在语音识别、自然语言处理、生物信息学和其他许多领域有广泛的应用。</p>
<p>隐马尔可夫模型主要包含以下几个组成部分：</p>
<ol>
<li><strong>隐状态集合</strong>：模型中不可直接观测到的内部状态的集合。</li>
<li><strong>观测集合</strong>：每个隐状态可以生成的观测数据的集合。</li>
<li><strong>状态转移概率矩阵</strong>：隐状态之间转移的概率，表示为一个矩阵。矩阵的每个元素 aij 表示从状态 i 转移到状态 j 的概率。</li>
<li><strong>观测概率分布</strong>：也称为发射概率，它表示在给定某个隐状态的情况下，生成某个观测值的概率。</li>
<li><strong>初始状态概率分布</strong>：模型在开始时每个隐状态的初始概率分布。</li>
</ol>
<p>隐马尔可夫模型的基本假设包括：</p>
<ul>
<li><strong>马尔可夫假设</strong>：每个隐状态仅依赖于它之前的一个隐状态。</li>
<li><strong>观测独立性假设</strong>：任何时刻的观测值仅依赖于当前的隐状态，与其他隐状态或观测值无关。</li>
</ul>
<p>隐马尔可夫模型的三个基本问题：</p>
<ol>
<li><strong>评估问题</strong>：给定模型参数和观测序列，计算观测序列出现的概率。这通常通过前向算法或后向算法解决。</li>
<li><strong>解码问题</strong>：给定模型参数和观测序列，找出最有可能产生这些观测的隐状态序列。这通常通过Viterbi算法解决。</li>
<li><strong>学习问题</strong>：给定观测序列，估计模型参数（状态转移概率、观测概率和初始状态概率），使得观测序列的概率最大。这可以通过Baum-Welch算法（一种特殊的期望最大化算法）来解决。</li>
</ol>
<h1 id="马尔可夫决策过程">马尔可夫决策过程</h1>
<p>马尔可夫决策过程（Markov Decision Process, MDP）是一种数学框架，用于形式化描述在不确定性环境中的决策制定问题。MDP 适用于那些决策结果部分依赖于决策者并且部分依赖于随机因素的情形。MDP 主要用于需要做出一系列决策的场景，如自动控制、经济学决策、强化学习等，目标是找到最优策略，以最大化长期奖励。</p>
<p>MDP 在马尔可夫链的基础上加入了“决策”的元素，引入了动作（Actions）、状态转移概率和奖励（Reward）。不仅关注状态的转移，还关注在给定状态下采取不同动作所导致的后果和收益。</p>
<p>一个 MDP 主要由以下四个元素组成：</p>
<ol>
<li>
<p><strong>状态（S）</strong>：描述决策过程中可能到达的所有状态的集合。每个状态提供了决策环境的完整描述。</p>
</li>
<li>
<p><strong>动作（A）</strong>：对于每个状态，都有一组可能的动作（或决策）。动作决定了状态转移的可能性。</p>
</li>
<li>
<p><strong>状态转移概率（P）</strong>：表示为 P(s'|s, a)，即在状态 s 下采取动作 a 后转移到状态 s' 的概率。这个概率捕捉了环境的动态性和不确定性。</p>
</li>
<li>
<p><strong>奖励（R）</strong>：函数 R(s, a, s') 表示从状态 s 采取动作 a 并转移到状态 s' 所得到的即时奖励（或成本）。奖励函数评估每个动作的即时效用。</p>
</li>
</ol>
<p>目标是找到一个<strong>策略</strong>（Policy），即从每个状态到动作的映射，以最大化某种性能标准，通常是预期收益的总和。这种性能标准可以是累积奖励的总和，也可能涉及对未来奖励的贴现（Discounting）。</p>
<p>在解决 MDP 问题时，通常涉及到两种主要的策略：</p>
<ul>
<li>
<p><strong>价值迭代（Value Iteration）</strong>：通过不断迭代更新状态值函数来逼近最优解，直到达到稳定状态。</p>
</li>
<li>
<p><strong>策略迭代（Policy Iteration）</strong>：包含两个步骤，策略评估（计算当前策略的值）和策略改进（基于当前值函数更新策略），这两个步骤交替进行直到策略收敛。</p>
</li>
</ul>
<p>MDP 提供了一种强大的框架来模拟和解决决策问题，尤其是在考虑时间和不确定性时。在强化学习中，智能体通过与环境的交互来学习最优策略，通常是在没有初始知识的情况下通过试错学习。</p>
<h1 id="马尔可夫随机场">马尔可夫随机场</h1>
<p>马尔可夫随机场（Markov Random Field，MRF），也被称为概率无向图模型，是一种用于建模多元随机变量联合分布的统计模型，其中变量间的关系用一个无向图来表示。在这个图中，每个节点代表一个随机变量，而边则表示变量之间的潜在依赖关系。</p>
<p>MRF 主要用于建模那些变量间具有空间或者其他形式相互依赖性的系统，广泛应用于图像处理、空间数据分析和计算机视觉等领域。MRF 的一个关键特性是局部性原理，即给定某个变量的邻居（在图中直接与之相连的节点）后，该变量条件独立于其它所有变量。这种性质被称为马尔可夫性质。</p>
<h3 id="关键概念">关键概念</h3>
<ul>
<li><strong>节点</strong>：图中的每个节点代表一个随机变量，可以是观测到的数据点或是隐藏变量。</li>
<li><strong>边</strong>：节点之间的边表示变量间的直接依赖关系。</li>
<li><strong>团和最大团</strong>：在图中，任何相互连接的节点集合称为团。不可能加入更多节点的团称为最大团。</li>
<li><strong>势函数</strong>：用于量化节点或节点组合的概率分布，通常定义在最大团上。势函数表达了变量之间的相互作用强度。</li>
</ul>
<h3 id="马尔可夫性质">马尔可夫性质</h3>
<p>MRF 的核心是马尔可夫性质，即在给定一个节点的邻居的情况下，该节点与图中其他节点条件独立。这意味着，节点的局部环境提供了足够的信息来预测该节点的行为，而无需考虑更远的节点。</p>
<h3 id="hammersley-clifford-定理">Hammersley-Clifford 定理</h3>
<p>这个定理是 MRF 理论中的一个关键结果，它指出：在满足一定条件下，一个分布可以被表示为一个马尔可夫随机场，如果且仅如果它可以被表示为一个图上的势函数的乘积形式。</p>
<h3 id="应用示例">应用示例</h3>
<p>在图像处理中，MRF 可以用于图像恢复、分割和纹理合成。比如，在噪声图像恢复中，每个像素点可以视为一个随机变量，相邻像素间的相关性可以用 MRF 来建模，以此推断最可能的干净图像。</p>
<h3 id="求解方法">求解方法</h3>
<p>MRF 的推断和学习通常是计算密集型的，常用的方法包括吉布斯采样（Gibbs Sampling）、模拟退火（Simulated Annealing）和置信传播（Belief Propagation）等。这些方法旨在通过迭代过程来近似地估计或优化目标函数，从而解决相关的最优化问题。</p>
<p><a href="https://blog.csdn.net/qq_40986693/article/details/104179088">Belief Propagation信念传播算法详解</a><br>
马尔科夫随机场提供了一种建模多变量依赖关系的框架，而信念传播算法则是一种在这种框架下进行有效推理的方法。通过在马尔科夫随机场上运用信念传播算法，可以进行高效的概率推断和决策。</p>
<h1 id="关于马尔可夫模型的问答">关于马尔可夫模型的问答</h1>
<h3 id="基础理解">基础理解</h3>
<ol>
<li>
<p>请解释什么是马尔可夫链以及它的基本性质。<br>
马尔可夫链是一种随机过程，其中下一个状态的概率仅依赖于当前状态，这被称为无记忆性。基本性质包括状态空间、初始状态概率和状态转移概率。</p>
</li>
<li>
<p>描述隐马尔可夫模型（HMM）及其与马尔可夫链的主要区别。<br>
HMM是一种统计模型，它用来描述观测序列背后的一个不可见的状态序列。与马尔可夫链不同，HMM包含隐状态和观测状态，且观测状态的概率分布依赖于隐状态。</p>
</li>
<li>
<p>马尔可夫链的“无记忆性”或“马尔可夫性”是什么意思？请给出一个实际应用的例子。<br>
马尔可夫性指的是系统的下一个状态仅依赖于当前状态，而与之前的历史无关。例如，天气模型中，明天是晴天还是雨天仅依赖于今天的天气，而不是之前的天气历史。</p>
</li>
<li>
<p>在隐马尔可夫模型中，什么是隐状态？什么是观测状态？<br>
在HMM中，隐状态是模型中不直接可见的内部状态，而观测状态是由隐状态生成的、可以直接观察到的状态。</p>
</li>
</ol>
<h3 id="数学与算法">数学与算法</h3>
<ol start="5">
<li>
<p>如何表示和计算马尔可夫链的状态转移概率矩阵？<br>
状态转移概率矩阵是一个方阵，其元素 aij 表示从状态 i 转移到状态 j 的概率。（邻接矩阵）</p>
</li>
<li>
<p>解释隐马尔可夫模型中的前向算法和后向算法的作用。<br>
前向算法用于计算在给定模型参数的情况下，观测序列出现的概率。后向算法也用于相似的目的，但是从序列的末尾开始计算。</p>
</li>
<li>
<p>什么是Viterbi算法？它在隐马尔可夫模型中用来解决什么问题？<br>
Viterbi算法是一种动态规划算法，用于找出最有可能产生给定观测序列的隐状态序列，在HMM中用于解决解码问题。</p>
</li>
<li>
<p>描述Baum-Welch算法在隐马尔可夫模型中的应用及其目的。<br>
Baum-Welch算法是一种特殊的EM算法，用于未知参数的HMM中。它通过迭代过程优化模型参数以最大化给定观测序列的概率。</p>
</li>
</ol>
<h3 id="应用与实践">应用与实践</h3>
<ol>
<li>描述一个使用隐马尔可夫模型的自然语言处理应用案例。<br>
在自然语言处理中，HMM可用于词性标注，其中隐状态代表单词的词性，观测状态代表实际的单词。</li>
</ol>
<h3 id="深度与挑战">深度与挑战</h3>
<ol>
<li>
<p>在处理非平稳时间序列数据时，马尔可夫链模型有哪些局限性，如何克服？<br>
马尔可夫链假设转移概率不随时间变化。对于非平稳数据，可以使用时间依赖的马尔可夫模型或通过增加状态来模拟时间变化的影响。</p>
</li>
<li>
<p>对于有大量状态的系统，隐马尔可夫模型的性能和可行性如何？存在哪些优化或替代方案？<br>
当状态空间很大时，HMM的计算复杂性会显著增加。可能的优化包括使用近似方法、约简状态空间或采用更高效的算法。</p>
</li>
<li>
<p>讨论在实际应用中构建和训练隐马尔可夫模型时可能遇到的挑战及其解决方案。<br>
实际应用中的挑战包括数据稀疏性、参数初始化以及模型选择。解决方案可能包括使用平滑技术、合理的启发式初始化策略和交叉验证来选择模型。</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[【ML】Softmax]]></title>
        <id>https://jeromezjl.github.io/post/ml-softmax/</id>
        <link href="https://jeromezjl.github.io/post/ml-softmax/">
        </link>
        <updated>2024-03-11T14:44:10.000Z</updated>
        <content type="html"><![CDATA[<p><img src="https://jeromezjl.github.io/post-images/1710168281390.png" alt="" loading="lazy"><br>
Softmax 函数如上图所示，分子 xi 是每个数据的值，将其指数化，将输出的数值拉开距离。分母是所有数据指数之和，这是一种概率形式，表达为样本占所有值的概率。<br>
它可以用作 Softmax 回归、Softmax 激活函数在神经网络中往往用在最后一层，特别是在处理分类问题时，将网络的原始输出转换为更直观的概率形式。</p>
<p><a href="https://zhuanlan.zhihu.com/p/105722023">一文详解Softmax函数</a></p>
<pre><code>Softmax 从字面上来说，可以分成soft和max两个部分。
max故名思议就是最大值的意思。Softmax的核心在于soft，而soft有软的含义，与之相对的是hard硬。
很多场景中需要我们找出数组所有元素中值最大的元素，实质上都是求的hardmax。
hardmax 是求一组数据中唯一的最大值，而 Softmax 是输出概率，可以自己选择概率最大的前几个值
</code></pre>
<p><a href="https://www.bilibili.com/video/BV18u411L7tU/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">什么是softmax回归，如何使用softmax回归，解决多分类任务</a></p>
<p>Softmax 激活函数：通常用于多分类问题的输出层（最后一层），将 logits（即最后一个线性层的输出）转换成概率分布。<br>
隐藏层激活函数不用 Softmax，用 ReLU、tanh 和 sigmoid 等，用于增加网络的非线性，帮助模型学习复杂的特征表示。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[【DL】Transformer]]></title>
        <id>https://jeromezjl.github.io/post/dl-transformer/</id>
        <link href="https://jeromezjl.github.io/post/dl-transformer/">
        </link>
        <updated>2024-03-11T09:23:50.000Z</updated>
        <content type="html"><![CDATA[<p><a href="https://www.bilibili.com/video/BV1MY41137AK/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">【Transformer模型】曼妙动画轻松学，形象比喻贼好记</a><br>
<a href="https://www.bilibili.com/video/BV1ih4y1J7rx/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">超强动画，一步一步深入浅出解释Transformer原理！</a><br>
<a href="https://www.bilibili.com/video/BV1Kq4y1H7FL?p=1">68 Transformer【动手学深度学习v2】</a><br>
<a href="https://zhuanlan.zhihu.com/p/338817680">Transformer模型详解（图解最完整版）</a><br>
<a href="https://zhuanlan.zhihu.com/p/403433120">【Transformer】10分钟学会Transformer | Pytorch代码讲解 | 代码可运行</a><br>
<a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">Transformer论文逐段精读【论文精读】</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/194308943">Seq2Seq模型介绍</a><br>
Transformer：Seq2Seq model with attention</p>
<h1 id="结构介绍">结构介绍</h1>
<figure data-type="image" tabindex="1"><img src="https://jeromezjl.github.io/post-images/1710839949031.png" alt="" loading="lazy"></figure>
<h2 id="attention">Attention</h2>
<p><a href="https://www.bilibili.com/video/BV1264y1i7R1/?spm_id_from=333.999.0.0&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">64 注意力机制【动手学深度学习v2】</a><br>
<a href="https://www.bilibili.com/video/BV1QW4y167iq/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">09 Transformer 之什么是注意力机制（Attention）</a><br>
<a href="https://www.bilibili.com/video/BV1q3411U7Hi/?spm_id_from=333.788.recommend_more_video.5&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">Attention、Transformer公式推导和矩阵变化</a><br>
和RNN的区别：RNN 是串行的，attention 是并行的。时序模型会占用很大的内存，而且容易遗忘前面学过的东西，transformer使用attention矩阵运算，使得数据可以并行计算，提升了效率</p>
<p>Query，Key，Value 的概念取自于信息检索系统，举个简单的搜索的例子来说。当你在某电商平台搜索某件商品（年轻女士冬季穿的红色薄款羽绒服）时，你在搜索引擎上输入的内容便是 Query。<br>
然后搜索引擎根据 Query 为你匹配 Key（例如商品的种类，颜色，描述等）。<br>
然后根据 Query 和 Key 的<code>相似度</code>，用 softmax 选择概率最大的，将结果和V相乘，得到 Value 中最匹配的内容。</p>
<p>相似度计算：QK 求内积（余弦相似度）</p>
<p>多头注意力机制是指，有n个自注意力模块，每个自注意力模块都有 QKV 三个矩阵。多头注意力的作用可以类比多卷积核，为了更好的提取特征</p>
<p>分为注意力、自注意力、带有mask的注意力<br>
注意力：简单的 QKV<br>
自注意力：QKV都来自于序列自己<br>
带有mask的注意力：attention 每一次都可以看见完整的输入，而训练 decoder 的时候，由于要预测 t 时刻后的输入，所以 decoder 的 attention 是带 mask 的，保证在 t 时间输入后不会看见 t 时间之后的输入，确保模型在生成每个词时只能依赖于它之前的词，从而保持自回归特性。</p>
<h2 id="feed-forward">Feed Forward</h2>
<p>全连接层</p>
<h2 id="addnorm">Add&amp;Norm</h2>
<p>Add 是一个残差块，和 ResNet 中的一样，使得网络能做很深<br>
<img src="https://jeromezjl.github.io/post-images/1710840452631.png" alt="" loading="lazy"><br>
为什么采用 layer norm 而不是 batch norm：因为在语言 embedding 中，每个样本的长度可能不一样，所以以 batch 为单位进行标准化会有无用信息；而 layer norm 是将每个特征做标准化，效果更好</p>
<h2 id="encoder-decoder">encoder-decoder</h2>
<p>encoder和decoder是一一对应的，且encoder的输出维度等于decoder的输入维度<br>
编码器的输出，将其作为解码器中第二层注意力的 key 和 value，其 query 来自目标序列</p>
<p>原文 encoder 和 decoder 都用了6个</p>
<p><strong>思维导图</strong><br>
<img src="https://jeromezjl.github.io/post-images/1686115237621.png" alt="" loading="lazy"><br>
<img src="https://jeromezjl.github.io/post-images/1686115244889.png" alt="" loading="lazy"><br>
<img src="https://jeromezjl.github.io/post-images/1686115251026.png" alt="" loading="lazy"><br>
<img src="https://jeromezjl.github.io/post-images/1686115255439.png" alt="" loading="lazy"></p>
<h1 id="变体">变体</h1>
<p>Transformer 模型自从在 &quot;Attention is All You Need&quot; 论文中被首次提出以来，已经孕育出许多变体，这些变体针对不同的应用场景和性能优化进行了设计。以下是一些比较著名的 Transformer 模型变体：</p>
<ol>
<li>
<p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: 由 Google 提出，BERT 通过双向训练的方式来更好地理解语言上下文，广泛应用于文本分类、命名实体识别、问答系统等领域。</p>
</li>
<li>
<p><strong>GPT (Generative Pre-trained Transformer)</strong>: OpenAI 开发的一系列模型，包括 GPT、GPT-2、GPT-3 等，主要用于文本生成任务，如文本续写、机器翻译、对话系统等。</p>
</li>
<li>
<p><strong>Transformer-XL</strong>: 这个变体通过使用一个特殊的序列建模方式解决了标准 Transformer 在处理长序列时的限制，它能够在长序列文本上获得更好的性能。</p>
</li>
<li>
<p><strong>XLNet</strong>: 结合了 Transformer-XL 和 GPT 的优点，使用了双向上下文和排列语言模型的训练方式，表现在很多自然语言处理任务中超越了 BERT。</p>
</li>
<li>
<p><strong>RoBERTa (Robustly Optimized BERT approach)</strong>: 是 BERT 的一个优化版本，通过更大规模的数据集和更长时间的训练，以及调整了一些超参数，来提高模型的性能。</p>
</li>
<li>
<p><strong>ALBERT (A Lite BERT)</strong>: 通过参数共享和因子化嵌入矩阵，显著减少了模型的大小，同时保持或超越了 BERT 的性能。</p>
</li>
<li>
<p><strong>T5 (Text-to-Text Transfer Transformer)</strong>: 将各种自然语言处理任务统一为一个文本到文本的框架，通过简化任务格式提高了模型的通用性和灵活性。</p>
</li>
<li>
<p><strong>Electra</strong>: 通过引入更有效率的预训练任务（类似于GAN的判别器任务），在较小的计算预算下达到或超过了 BERT 的性能。</p>
</li>
<li>
<p><strong>DeBERTa (Decoding-enhanced BERT with Disentangled Attention)</strong>: 通过改进 BERT 的注意力机制，引入解耦注意力和增强的掩码解码器，提高了模型处理自然语言理解任务的能力。</p>
</li>
<li>
<p><strong>ViT (Vision Transformer)</strong>: 将 Transformer 应用于图像分类任务，通过将图像分割成多个小块（patches）并将它们作为序列输入到 Transformer 中，展示了 Transformer 架构在非NLP任务上的潜力。</p>
</li>
</ol>
]]></content>
    </entry>
</feed>