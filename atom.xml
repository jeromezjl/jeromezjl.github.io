<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://localhost:4000</id>
    <title>Jerome</title>
    <updated>2022-12-16T11:22:14.139Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="http://localhost:4000"/>
    <link rel="self" href="http://localhost:4000/atom.xml"/>
    <subtitle>Jerome&apos;s blog</subtitle>
    <logo>http://localhost:4000/images/avatar.png</logo>
    <icon>http://localhost:4000/favicon.ico</icon>
    <rights>All rights reserved 2022, Jerome</rights>
    <entry>
        <title type="html"><![CDATA[【线性代数】矩阵的特征值和特征向量]]></title>
        <id>http://localhost:4000/post/xian-xing-dai-shu-ju-zhen-de-te-zheng-zhi-he-te-zheng-xiang-liang/</id>
        <link href="http://localhost:4000/post/xian-xing-dai-shu-ju-zhen-de-te-zheng-zhi-he-te-zheng-xiang-liang/">
        </link>
        <updated>2022-12-05T10:23:51.000Z</updated>
        <content type="html"><![CDATA[<p>所谓凡事见微知著，由简推繁，要理解多维矩阵的特征值和特征向量的意思，我们不如从二维推起。<br>
矩阵，又称为矩阵空间，矩阵包含空间信息。1x2 的矩阵，是二维空间上的矩阵</p>
<p>若非零向量 x 满足 Ax = λx，则 λ 为特征值，x 为特征向量。<br>
特征向量在经过矩阵 A 的变换后，不改变，只需要乘上一个常数 λ</p>
<p>对于矩阵特征值和特征向量公式的理解：一个矩阵点乘一个向量就相当于对该向量进行旋转或者拉伸等一系列线性变换。在我们求解矩阵的特征值和特征向量的时候就是要求解矩阵 A，A能够使得这些特征向量只发生伸缩变换，而变换的效果等价于特征向量与某个常量 λ 相乘。<br>
在二维空间理解一下这个式子的含义。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SQL 复习]]></title>
        <id>http://localhost:4000/post/sql-qi-zhong-fu-xi/</id>
        <link href="http://localhost:4000/post/sql-qi-zhong-fu-xi/">
        </link>
        <updated>2022-10-27T09:14:16.000Z</updated>
        <content type="html"><![CDATA[<p>复习策略：<br>
看网络资料的数据库复习<br>
弄懂知识点，然后做一下里面的题</p>
<h1 id="第三章-sql">第三章 SQL</h1>
<p>熟练编写 sql 语句，创建、删除语句也要看，数据类型</p>
<h1 id="第四章">第四章</h1>
<p>◼ Join Expressions（连接表达式）<br>
◼ Views 视图<br>
◼ Transactions<br>
◼ Integrity Constraints<br>
◼ SQL Data Types and Schemas<br>
◼ Authorization<br>
◼ Security in SQL</p>
<p>Join 参见 sql 的总结，natural join，inner join 等</p>
<h1 id="第五章-高级-sql">第五章 高级 SQL</h1>
<p>这章期中没考</p>
<p>使用程序设计语言访问数据库<br>
通过两种方式从通用编程语言访问 SQL：<br>
动态 SQL：通过函数或方法<br>
嵌入式 SQL：在编译时全部确定</p>
<p>JDBC 标准定义了 java 和 SQL 的 API</p>
<h1 id="第六章-关系代数">第六章 关系代数</h1>
<p><a href="https://blog.csdn.net/quinnnorris/article/details/70739094">SQL 形式化语言——关系代数</a><br>
重点是如何将sql语句转化为关系代数</p>
<p>选择运算 select operation σ<br>
选择关系instructor中属于物理系的元组：<br>
σ dept_name=“Physics”(instructor)<br>
选择关系 instructor 中 salary&gt;90000 的元组：<br>
σ salary&gt;90000 (instructor)<br>
⋀ (and), ⋁ (or), (not)</p>
<p>投影运算 project operation  ∏<br>
用于选择表中想要的属性<br>
∏ name, salary (instructor)<br>
找出物理系中所有教师的名字：<br>
∏ name (σ dept_name=“Physic’ (instructor))</p>
<p>并运算 union operation ∪<br>
找出开设在 2009 年球季或者 2010 年春季学期，或者二者皆开的课程集合<br>
∏course_id (σ semester=“Fall” Λ year=2009 (section)) ∪<br>
∏course_id (σ semester=“Spring” Λ year=2010 (section))</p>
<p>集合差运算 Set Difference Operation -<br>
找出开设在 2009 年秋季但不开设在 2010 年春季的课程<br>
∏course_id (σ semester=“Fall”Λ year=2009(section))-<br>
∏course_id (σ semester=“Spring”Λ year=2010(section))</p>
<p>笛卡尔积运算 Cartesian-Product Operation x<br>
∏ name (σ dept_name=“Physic’ (instructor x teaches))</p>
<p>更名运算 Rename Operation<br>
选出 instructor 中 salary 最高的：<br>
σ instructor.salary &lt; d.salary (instructor ╳ ρ d(instructor))</p>
<p>select T.salary<br>
from instructor T, instructor S<br>
where T.salary &gt; S.salary</p>
<h1 id="第七章-e-r-图">第七章 E-R 图</h1>
<p>非ER关系中，两个实体关联，每个实体中可能会有对方实体的主码，从而产生冗余<br>
在ER关系中，如果一个实体的属性是另一个实体的主码，则删除该属性。二者的关联会在联系集里表达。（P183）</p>
<p>大学实体集及属性（P154 输入183）</p>
<p>大学 E-R 图（P159 输入188）</p>
<p>E-R 图图形含义（P172 输入 201）</p>
<p>练习题（P179 输入208）</p>
<p>弱实体集：如果保留某些属性，则在和别的集合关联时，会造成属性的冗余。所以我们选择删掉该属性。但删掉后，当前剩余的属性就不能保证唯一标识元素。这样的实体集称为弱实体集。弱实体集必须与强实体集关联才有意义。</p>
<p>E-R 图中，双线表示有且仅有一个相关的对象<br>
箭头：A → B 表明 每个 A 至多有一个 B</p>
<p>构造关系表时，如果有多个候选键，最好选取数值型（int, float）候选键作为关系表主键，便于提高基于主键的查询速度<br>
—不要选字符串型属性，如varchar、datetime<br>
—e.g. studentname, instructorName</p>
<p>双菱形表示弱实体集的标识性联系集</p>
<p>构建 E-R 图步骤<br>
从关系集入手，分别判断其关联的两个实体集的对应关系。看是采用双线、箭头，还是单线。<br>
判断弱实体集：看关联的两个实体集的属性，是否有相同的，如果有，去掉其中一个的属性，将其变为弱实体集，然后将关系变为双菱形。</p>
<h1 id="第八章-关系数据库的设计">第八章 关系数据库的设计</h1>
<p>◼ Features of Good Relational Design<br>
◼ Atomic Domains and First Normal Form<br>
◼ Decomposition Using Functional Dependencies<br>
◼ Functional Dependency Theory<br>
◼ Algorithms for Functional Dependencies</p>
<p><strong>好的关系设计的特点</strong><br>
取决于E-R图质量<br>
是否有重复，是否可以很好的表示所有信息<br>
删除和更新问题</p>
<p><a href="https://www.cnblogs.com/sky20080101/articles/8445061.html">原子性和第一范式</a></p>
<h1 id="第十章-数据存储和文件结构">第十章 数据存储和文件结构</h1>
<p>◼ File organization (at physical level, §10.5)<br>
◼ Organization of records in files (at logical level, §10.6 ),<br>
i.e. file structures<br>
◼ Data-dictionary Storage (§10.7)<br>
◼ Data Buffer (§10.8)</p>
<p><strong>10.5 文件组织</strong></p>
<h1 id="第十一章-索引">第十一章 索引</h1>
<p>搜索码：用于在文件中查找记录的属性或属性集<br>
索引<br>
什么是<br>
聚集索引，非聚集索引，稠密索引，稀疏索引，</p>
<p>每个索引对数据增删改查是否有影响，</p>
<p>索引创建在搜索码（属性）上面，可提高检索速度<br>
要会说明原因</p>
<p>在除了查询之外，delete update insert 时如何提高效率，搜索码<br>
索引也有空间、时间开销，增加/删除，所以不一定能对增加删除修改起到提高效率的作用<br>
也可能拖慢速度</p>
<h1 id="第十二章-查询处理">第十二章 查询处理</h1>
<p>熟悉关系代数的表示<br>
<a href="https://blog.csdn.net/bianyamei/article/details/89491358">启发式查询树优化实例</a><br>
<a href="https://www.bilibili.com/video/BV1Fp4y1a7Cs/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">B站讲解 数据库语法树优化</a></p>
<p>步骤<br>
1、写出关系代数表达式<br>
2、画出查询树<br>
3、选择关系下移<br>
4、投影下移，注意投影要在选择关系之上；每个自然连接前面必须有投影</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ML/DL 中常见的英文]]></title>
        <id>http://localhost:4000/post/mldl-zhong-chang-jian-de-ying-wen/</id>
        <link href="http://localhost:4000/post/mldl-zhong-chang-jian-de-ying-wen/">
        </link>
        <updated>2022-10-25T14:06:00.000Z</updated>
        <content type="html"><![CDATA[<p>parameters / params：参数<br>
epochs：总训练次数，使用全部数据训练一次是一个epochs<br>
learning rate：学习率<br>
optimize：优化<br>
regularization：正则化<br>
normalize：归一化、标准化<br>
Regularization： 正则化</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[以梯度下降（Gradient Descent）展开的优化器总结]]></title>
        <id>http://localhost:4000/post/ti-du-xia-jiang-gradient-descent/</id>
        <link href="http://localhost:4000/post/ti-du-xia-jiang-gradient-descent/">
        </link>
        <updated>2022-10-25T12:57:09.000Z</updated>
        <content type="html"><![CDATA[<p>本篇关键词：损失函数/代价函数/误差函数、梯度下降、学习率、Momentum（动量）</p>
<p><a href="https://zhuanlan.zhihu.com/p/68468520">梯度下降</a><br>
<a href="https://zhuanlan.zhihu.com/p/357963858">随机梯度下降</a></p>
<p>大多数机器学习模型都会有一个损失函数。比如常见的 均方误差 （Mean Squared Error —— MSE）损失函数，其输出值为 模型的输出值和实际值的偏差。<br>
损失函数的输出值越小，模型精度越高。我们用梯度下降的方法最小化损失函数。</p>
<p>梯度下降（Gradient Descent）：使用所有样本进行梯度下降<br>
小批量样本梯度下降（Mini Batch GD）：使用小批量样本进行梯度下降<br>
随机梯度下降（Stochastic GD）：使用一个样本进行梯度下降</p>
<p><a href="https://www.zhihu.com/question/395685065/answer/2535950728">怎么通俗易懂的理解SGD中Momentum的含义？</a></p>
<p>SGD 的改进形式 —— Adam</p>
<p><a href="https://www.zhihu.com/question/42115548/answer/1636798770">SGD有多种改进的形式(RMSprop,Adadelta等),为什么大多数论文中仍然用SGD?</a></p>
<p><a href="https://blog.csdn.net/qq_42109740/article/details/105401197">深度学习各类优化器详解（动量、NAG、adam、Adagrad、adadelta、RMSprop、adaMax、Nadam、AMSGrad）</a></p>
<p><a href="https://blog.csdn.net/qq_36589234/article/details/89330342">PyTorch学习之 torch.optim 的6种优化器及优化算法介绍</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[KPL 语言]]></title>
        <id>http://localhost:4000/post/kpl-yu-yan/</id>
        <link href="http://localhost:4000/post/kpl-yu-yan/">
        </link>
        <updated>2022-10-16T09:38:19.000Z</updated>
        <content type="html"><![CDATA[<h1 id="编译过程">编译过程</h1>
<p>编译 package Hello 生成 Hello.s</p>
<pre><code class="language-C">kpl Hello
</code></pre>
<p>编译 Hello.s 生成 Hello.o</p>
<pre><code class="language-C">asm Hello.s
</code></pre>
<p>combine all of the &quot;.o&quot; object files into an executable file</p>
<pre><code class="language-c">lddd System.o Hello.o Runtime.o -o Hello
</code></pre>
<p>with the &quot;-o&quot; option , the new file will be named &quot;Hello&quot; or will be &quot;a.out&quot;</p>
<p>To run the package &quot;Hello&quot;</p>
<pre><code class="language-C">blitz -g Hello
</code></pre>
<p>&quot;-g&quot; means run it directly</p>
<p>After execution completes,enter &quot;q&quot; to quit</p>
<h1 id="the-header-and-code-files">The Header and Code Files</h1>
<p>A program is made of several packages and each package is described by a header file and a code file</p>
<p>The header file is the specification for the package. It provides the external interface to that package,giving all information other packages will need about what is in the package. In the Hello-World example, the file “Hello.h” specifies the package will contain a function called “main” and tells what parameters this function takes and returns. (The main function takes no parameters and returns no results.)</p>
<p>The code file contains the implementation details for the package. All executable code appears in the code file. In the Hello-World example, the “Hello.c” file contains the actual code for the main function</p>
<h1 id="编译过程-2">编译过程</h1>
<p>直接输入 make，在文件夹内会根据 makefile 中的规则编译所有文件<br>
再次输入 blitz -g os 运行所有代码</p>
<p>https://github.com/ayushishri/OS-Blitz-Labs</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MNIST 手写数字识别]]></title>
        <id>http://localhost:4000/post/mnist-shou-xie-shu-zi-shi-bie/</id>
        <link href="http://localhost:4000/post/mnist-shou-xie-shu-zi-shi-bie/">
        </link>
        <updated>2022-10-14T07:23:30.000Z</updated>
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/m0_58092763/article/details/125631991?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-125631991-blog-112980305.t5_landing_title_tags&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-125631991-blog-112980305.t5_landing_title_tags&amp;utm_relevant_index=2">手把手实战PyTorch手写数据集MNIST识别项目全流程</a></p>
<p><a href="https://blog.csdn.net/wqy1837154675/article/details/108003698">pytorch读取MNIST数据集并显示</a></p>
<h1 id="理论">理论</h1>
<p><strong>数据读取和预处理</strong><br>
CUDA：显卡驱动，有了CUDA显卡才能进行复杂运算<br>
<a href="https://blog.csdn.net/wuzhongqiang/article/details/105499476">数据读取 DataLoader 和 图像预处理 transforms 详解（这篇值得反复阅读）</a><br>
<a href="https://www.jianshu.com/p/22c50ded4cf7">深度学习 | 三个概念：Epoch, Batch, Iteration</a><br>
<a href="https://blog.csdn.net/weixin_43135178/article/details/115133115">使用 transforms.Compose() 将图像变换方法整合在一起</a><br>
<a href="https://blog.csdn.net/qq_37555071/article/details/107532319">torchvision.transforms 对有限的图片数据进行各种变换，如缩小或者放大图片的大小、对图片进行水平或者垂直翻转等，这些都是数据增强的方法。</a><br>
<a href="https://www.pudn.com/news/6355f71da4b7e43a5ea8bc09.html#%E3%80%90%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE%E3%80%91">使用sklearn实现对数据集划分，从而进行交叉验证</a></p>
<p><strong>可视化工具包</strong><br>
<a href="https://blog.csdn.net/zkp_987/article/details/81748098">以进度条形式可视化迭代器运行的包：tqdm</a></p>
<p><a href="https://www.bilibili.com/video/BV1K64y1Q7wu?p=3&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">李沐 Softmax 函数</a></p>
<p>对 softmax 回归的感性理解：<br>
线性回归模型多输入，单输出，而 softmax 多输入，多输出。输出结果为输入的向量所属类别，由于类别有多个，所以多输出。输出概率最大的为所属类别。但由于直接输出的概率值 总和不一定为1，且可能有负值，所以对多个输出结果进行 softmax 方法的计算，从而使得输出结果为 总和为1，且均为正值的概率值。</p>
<h1 id="实现">实现</h1>
<pre><code class="language-python"># 导入库
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from sklearn.model_selection._split import KFold
</code></pre>
<pre><code class="language-python"># 定义超参数
BATCH_SIZE = 128  # 每批处理的数据量
DEVICE = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)  # 用CPU还是GPU训练
EPOCHS = 10  # 定义总训练次数
k_split_value = 5  # 定义 5 折交叉验证
</code></pre>
<pre><code class="language-python"># 定义图像处理方法
tranform = transforms.Compose([
    transforms.ToTensor(),  # 将图片转换成Tensor
    transforms.Normalize((0.1307,), (0.3081,))  # 进行数据归一化处理
])
</code></pre>
<pre><code class="language-python"># 下载、加载数据集
from torch.utils.data import DataLoader

train_data = datasets.MNIST(root=&quot;./MNIST&quot;,
                            train=True,
                            transform=tranform,
                            download=False)  # 注，如果已经下载了一遍，也需要用该命令设置data的路径

test_data = datasets.MNIST(root=&quot;./MNIST&quot;,
                           train=False,
                           transform=tranform,
                           download=False)

# 将测试集和训练集合并，以便后续对数据集进行分割
dataFold = torch.utils.data.ConcatDataset([train_data, test_data])
</code></pre>
<pre><code class="language-python"># 构建网络模型，使用 AlexNet
class AlexNet(nn.Module):
    def __init__(self):
        super(AlexNet, self).__init__()

        # 由于MNIST为28x28， 而最初AlexNet的输入图片是227x227的。所以网络层数和参数需要调节
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # AlexCONV1(3,96, k=11,s=4,p=0)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # AlexPool1(k=3, s=2)
        self.relu1 = nn.ReLU()

        # self.conv2 = nn.Conv2d(96, 256, kernel_size=5,stride=1,padding=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # AlexCONV2(96, 256,k=5,s=1,p=2)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # AlexPool2(k=3,s=2)
        self.relu2 = nn.ReLU()

        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # AlexCONV3(256,384,k=3,s=1,p=1)
        # self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)
        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)  # AlexCONV4(384, 384, k=3,s=1,p=1)
        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)  # AlexCONV5(384, 256, k=3, s=1,p=1)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # AlexPool3(k=3,s=2)
        self.relu3 = nn.ReLU()

        self.fc6 = nn.Linear(256 * 3 * 3, 1024)  # AlexFC6(256*6*6, 4096)
        self.fc7 = nn.Linear(1024, 512)  # AlexFC6(4096,4096)
        self.fc8 = nn.Linear(512, 10)  # AlexFC6(4096,1000)
        self.dropout = nn.Dropout(p=0.5)

    def forward(self, x):
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.relu1(x)
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.relu2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.conv5(x)
        x = self.pool3(x)
        x = self.relu3(x)
        x = x.view(-1, 256 * 3 * 3)  # Alex: x = x.view(-1, 256*6*6)
        x = self.fc6(x)
        x = self.dropout(x)
        x = F.relu(x)
        x = self.fc7(x)
        x = self.dropout(x)
        x = F.relu(x)
        x = self.fc8(x)
        return x
</code></pre>
<pre><code class="language-python"># 定义优化器
model = AlexNet().to(DEVICE)
optimizer = optim.Adam(model.parameters())  # 使用 Adam 优化器
</code></pre>
<pre><code class="language-python"># 定义训练方法
def train_model(model, device, train_loader, optimizer, epoch):
    model.train()  # PyTorch 提供的训练方法
    for batch_index, (data, label) in enumerate(train_loader):
        # 部署到DEVICE
        data, label = data.to(device), label.to(device)
        # 梯度初始化为0
        optimizer.zero_grad()
        # 训练后的结果
        output = model(data)
        # 计算损失（针对多分类任务交叉熵，二分类用sigmoid）
        loss = F.cross_entropy(output, label)
        # 找到最大概率的下标
        pred = output.argmax(dim=1)
        # 反向传播Backpropagation
        loss.backward()
        # 参数的优化
        optimizer.step()
        if batch_index % 3000 == 0:
            print(&quot;Train Epoch : {} \t Loss : {:.6f}&quot;.format(epoch, loss.item()))
</code></pre>
<pre><code class="language-python"># 定义测试方法
def test_model(model, device, test_loader):
    # 模型验证
    model.eval()
    # 统计正确率
    correct = 0.0
    # 测试损失
    test_loss = 0.0
    with torch.no_grad():  # 不计算梯度，不反向传播
        for data, label in test_loader:
            data, label = data.to(device), label.to(device)
            # 测试数据
            output = model(data)
            # 计算测试损失
            test_loss += F.cross_entropy(output, label).item()
            # 找到概率值最大的下标
            pred = output.argmax(dim=1)
            # 累计正确率
            correct += pred.eq(label.view_as(pred)).sum().item()
        test_loss /= len(test_loader.dataset)
        print(&quot;Test —— Average loss : {:.4f}, Accuracy : {:.3f}\n&quot;.format(test_loss,
                                                                          100.0 * correct / len(test_loader.dataset)))
</code></pre>
<pre><code class="language-python"># 对数据进行 K 折交叉划分并且调用前面的方法训练模型
def KFold_and_Train(k_split_value):
    counter = 1  # 自定义一个交叉验证计数器
    kf = KFold(n_splits=k_split_value, shuffle=True, random_state=0)  # 定义K折交叉验证的方法
    for train_index, test_index in kf.split(dataFold):  # 使用kf方法将dataFold分成测试集和验证集
        print(f&quot;第{counter}次交叉验证&quot;)
        counter += 1  # 计数器自增1

        # get train, val
        train_fold = torch.utils.data.dataset.Subset(dataFold, train_index)
        test_fold = torch.utils.data.dataset.Subset(dataFold, test_index)

        # package type of DataLoader
        train_loader = torch.utils.data.DataLoader(dataset=train_fold, batch_size=BATCH_SIZE, shuffle=True)
        test_loader = torch.utils.data.DataLoader(dataset=test_fold, batch_size=BATCH_SIZE, shuffle=True)

        # training and test
        for epoch in range(EPOCHS):
            train_model(model, DEVICE, train_loader, optimizer, epoch)
            test_model(model, DEVICE, test_loader)
</code></pre>
<pre><code class="language-python"># 调用函数
KFold_and_Train(k_split_value)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Novelai 使用]]></title>
        <id>http://localhost:4000/post/novelai-shi-yong/</id>
        <link href="http://localhost:4000/post/novelai-shi-yong/">
        </link>
        <updated>2022-10-13T06:07:39.000Z</updated>
        <content type="html"><![CDATA[<p><a href="https://www.bilibili.com/video/BV1EV4y1L7dX/?vd_source=3d9ada7d42c971c0c3f04a22270daf33">教程</a></p>
<p>启动：浏览器输入域名：127.0.0.1:6969</p>
<p><a href="https://zhuanlan.zhihu.com/p/572865961">参数教程</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[如何用英语思考？]]></title>
        <id>http://localhost:4000/post/ru-he-yong-ying-yu-si-kao/</id>
        <link href="http://localhost:4000/post/ru-he-yong-ying-yu-si-kao/">
        </link>
        <updated>2022-09-26T07:37:07.000Z</updated>
        <content type="html"><![CDATA[<p><a href="https://www.bilibili.com/video/BV1TD4y1q7u9/?spm_id_from=333.999.0.0&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">b站</a></p>
<h1 id="reading">Reading</h1>
<p>阅读帮助建立特定单词的特定语境</p>
<h1 id="new-words">New Words</h1>
<p>看全英文的单词解释，用英语解释英语，形成英语思维。如果用中文，那么还是在用中文思考。<br>
学习一个单词的所有变形，尽量学习所有意思<br>
在语境中学习</p>
<p><a href="https://www.vocabulary.com/">vocabulary 英文单词查询网站</a></p>
<p>#Idioms<br>
在正确的情况下正确使用谚语</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[电脑操作]]></title>
        <id>http://localhost:4000/post/dian-nao-cao-zuo/</id>
        <link href="http://localhost:4000/post/dian-nao-cao-zuo/">
        </link>
        <updated>2022-09-17T16:13:10.000Z</updated>
        <content type="html"><![CDATA[<p><a href="https://www.bilibili.com/video/BV1T54y177W2/?vd_source=3d9ada7d42c971c0c3f04a22270daf33">一键禁用电脑键盘</a></p>
<p><a href="https://baijiahao.baidu.com/s?id=1634287239598842140">用 Excel 打开 CSV 格式文件乱码</a></p>
<p>全半角切换：<br>
电脑设置中启用 shift+空格切换全半角<br>
使用全角模式，在markdown中输入空格，则会显示空格</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[六级]]></title>
        <id>http://localhost:4000/post/liu-ji/</id>
        <link href="http://localhost:4000/post/liu-ji/">
        </link>
        <updated>2022-09-16T07:09:36.000Z</updated>
        <content type="html"><![CDATA[<p><a href="https://www.hjenglish.com/new/p1334552/">分值分布</a><br>
<a href="https://zhuanlan.zhihu.com/p/373774215">时间安排</a><br>
<a href="https://zhenti.burningvocabulary.com/login?uid=632280fbaafe55b3c209ddb6">做题的神仙网站（随时更新，能看单词意思）</a></p>
<p>高级词同义替换；使用复杂句式；灵活使用时态、语态</p>
<h1 id="六级作文类型">六级作文类型</h1>
<pre><code class="language-c">第一段
1~2 句提出问题，3过渡句

第二段
论点1 论据1
论点2 论据2
论点3 论据3

第三段
重申观点
提出希望
</code></pre>
<p>观点对立 / 分析利弊<br>
Some people think its bad others think its good</p>
<p>议论型</p>
<p><code>作文模板</code><br>
第一段</p>
<pre><code class="language-c">In the contemporary world 当今世界
there is no consensus among colledge students about the choice of career 没有定论
this proverb reveals that 
according to a recent survey conducted by
with the rapid growth of online shopping 
The popularity of smartphones makes the smartphone addiction increasingly commonplace

引起社会广泛关注
this phenomenon has become a worldwide social problem
... has stirred wide social concern

 反应了很多人对于...的看法
the concept of ... is gaining more popularity and striking deeper roots in people's heart
it reflects a number of people's concepts about ...
people come to realize that it is of practical value to stick to the famous saying:

引出谚语
As the well-known proverb goes
Though it is the insight summarized by our forefathers, it is correct and applicable in many case today.

解释谚语
The message it conveys is that

过渡句
Among countless reasons which support my view, there are three conspicuous aspects as follow
</code></pre>
<p>第二段</p>
<pre><code class="language-c">First and foremost, there is no doubt that ...
Moreover no one can deny that 
Last but not least, i firmly believe that


</code></pre>
<p>第三段</p>
<pre><code class="language-c">in conclusion / To sum up / All in all
while stressing... we should not neglect...

joint efforts should be made to promote...
we spare no efforts to do
if we try our upmost to do
the more.. the more...

our future will be both hopeful and rosy
The pursuit of ... should never be ceased 追逐...的步伐从不应该停止

</code></pre>
<p><code>高级短语</code></p>
<pre><code class="language-c">as it literally shows
the reason why
the overwhelming majority of 绝大多数
be reluctant to 不情愿
of practical value to do 有实际意义做
stick to 
reflect 反映
remarkable
reveal 揭示
tend to 
a high population of 56 percent of people
Thus
its urgent to do
as is universally acknowledged 
be of great significance
increasingly important
people believe that
extend to
the popularity of 
exerts multiple adverse impacts 造成多重不利影响
individual
all-pervasive 无所不在
quit a few peoper would deem that ... 少数人会认为
in contrast
intriguing 有趣
modern citizens
enhance
its obvious that
it's a high time that
joint efforts from relative authorities, teachers and students are required
Do bear in mind: 一定要记住
attach importance to
reinforce
one should adapt himself to the environment
</code></pre>
<h1 id="阅读">阅读</h1>
<p>找和题干最匹配的句子，句意、同义词替换<br>
读句子抓主谓宾，副词、形容词实在不知道可以先不卡看<br>
对比相似选项，看哪个和原文最匹配，防止误选</p>
<p><strong>段落匹配</strong><br>
先读题干句子，抓关键词<br>
看首尾句，明确段意<br>
抓句子关键词，带回段落定位</p>
]]></content>
    </entry>
</feed>