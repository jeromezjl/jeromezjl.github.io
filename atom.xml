<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jeromezjl.github.io</id>
    <title>Jerome</title>
    <updated>2023-02-13T04:29:47.341Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jeromezjl.github.io"/>
    <link rel="self" href="https://jeromezjl.github.io/atom.xml"/>
    <subtitle>Jerome&apos;s blog</subtitle>
    <logo>https://jeromezjl.github.io/images/avatar.png</logo>
    <icon>https://jeromezjl.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, Jerome</rights>
    <entry>
        <title type="html"><![CDATA[C++查缺补漏]]></title>
        <id>https://jeromezjl.github.io/post/ccha-que-bu-lou/</id>
        <link href="https://jeromezjl.github.io/post/ccha-que-bu-lou/">
        </link>
        <updated>2023-01-04T05:36:25.000Z</updated>
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/yourfriendyo/article/details/119544221">C语言详解：结构体</a><br>
<a href="https://blog.csdn.net/m0_52902391/article/details/120614881">模板类/模板函数 template 的用法(超详细)</a><br>
template：模板</p>
<h1 id="循环">循环</h1>
<p>while (cin &gt;&gt; x){}<br>
cin函数，输入NULL时返回0，输入其他值返回它的地址</p>
<h1 id="指针">指针</h1>
<p><a href="https://blog.csdn.net/weixin_39640298/article/details/84900326">C++指针详解</a><br>
&amp;：取地址<br>
*：取地址里面的值<br>
<a href="https://www.w3cschool.cn/cpp/cpp-passing-arrays-to-functions.html">向函数传递数组的写法</a><br>
用“-&gt;”的情况：<br>
A是一个类 class / struct<br>
p是一个指向A类型的指针，那么用p访问A中的成员变量和函数时，使用p-&gt;x，p-&gt;f(x)来访问。</p>
<h1 id="类和对象">类和对象</h1>
<p>类中不带返回类型的函数为构造函数。造函数可用于为某些成员变量设置初始值。</p>
<p><strong>C++三种常见的实例化方法</strong></p>
<ol>
<li>静态实例化：在程序的全局或静态作用域中定义并初始化一个对象，该对象在程序整个生命周期内只存在一个实例。</li>
<li>堆实例化：使用 new 运算符在动态存储区域中分配内存并实例化一个对象。这样创建的对象在程序运行期间一直存在，直到使用 delete 运算符显式释放其内存。</li>
<li>栈实例化：在函数或代码块的作用域中定义并实例化一个对象，该对象的生命周期与所在的函数或代码块相同。对象在离开该作用域时自动销毁。</li>
</ol>
<p><strong>静态实例化的例子</strong></p>
<pre><code class="language-c++">class MyClass {
public:
    MyClass() : m_x(0) {}
    void setX(int x) { m_x = x; }
    int getX() const { return m_x; }
private:
    int m_x;
};

MyClass globalObject;  // 全局实例

int main() {
    static MyClass staticObject;  // 静态实例
    globalObject.setX(5);
    staticObject.setX(10);
    int globalX = globalObject.getX(); // globalX = 5
    int staticX = staticObject.getX(); // staticX = 10
    // ...
    return 0;
}
</code></pre>
<p>上面的例子中，通过调用 setX 和 getX 来对全局对象 globalObject 和静态对象 staticObject 的成员变量进行访问。</p>
<p><strong>堆实例化的例子</strong></p>
<pre><code class="language-c++">class MyClass {
public:
    MyClass() : m_x(0) {}
    void setX(int x) { m_x = x; }
    int getX() const { return m_x; }
private:
    int m_x;
};

int main() {
    MyClass* heapObject = new MyClass;  // 堆实例
    heapObject-&gt;setX(5);
    int heapX = heapObject-&gt;getX(); // heapX = 5
    // ...
    delete heapObject;
    return 0;
}
</code></pre>
<p>上面的例子中，通过调用 setX 和 getX 来对堆对象 heapObject 的成员变量进行访问。</p>
<p><strong>栈实例化的例子</strong></p>
<pre><code class="language-c++">class MyClass {
public:
    MyClass() : m_x(0) {}
    void setX(int x) { m_x = x; }
    int getX() const { return m_x; }
private:
    int m_x;
};

void someFunction() {
    MyClass stackObject;  // 栈实例
    stackObject.setX(5);
    int stackX = stackObject.getX(); // stackX = 5
    // ...
}

int main() {
    someFunction();
    return 0;
}
</code></pre>
<p>上面的例子中，通过调用 setX 和 getX 来对栈对象 stackObject 的成员变量进行访问。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN网络]]></title>
        <id>https://jeromezjl.github.io/post/gan-wang-luo/</id>
        <link href="https://jeromezjl.github.io/post/gan-wang-luo/">
        </link>
        <updated>2022-12-26T11:37:25.000Z</updated>
        <content type="html"><![CDATA[<p>训练循环顺序：<br>
步骤1<br>
生成器不动，生成器产生一批假图片，再拿取一批真实图片，喂给判别器，训练判别器。<br>
我们希望判别器对假图片打上标签0，对真图片打上标签1<br>
步骤2<br>
判别器不动，训练生成器</p>
<p><a href="https://blog.csdn.net/weixin_44887621/article/details/120535309">pytorch保存图片 save_image ，读取图片</a></p>
<p>训练GAN网络训练多少次比较合适？<br>
最好使用一种称为 &quot;early stopping&quot; 的方法，即在模型在验证集上的表现开始下降时停止训练。这样可以避免模型出现过拟合的情况。</p>
<p>另外，也可以使用 &quot;learning rate decay&quot; 的方法来防止模型出现过拟合的情况。这种方法的原理是，在训练过程中，会逐渐降低学习率，以便模型能够更加稳定地收敛到最优解。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[数据归一化处理]]></title>
        <id>https://jeromezjl.github.io/post/shu-ju-gui-yi-hua-chu-li/</id>
        <link href="https://jeromezjl.github.io/post/shu-ju-gui-yi-hua-chu-li/">
        </link>
        <updated>2022-12-25T11:59:16.000Z</updated>
        <content type="html"><![CDATA[<p>数据归一化是一种常用的预处理技术，它的目的是将不同范围的数据转换为相同范围的数据，这样可以使不同的数据具有相似的分布和取值范围。</p>
<p>常见的数据归一化方法有以下几种：</p>
<p><strong>最值归一化（Min-Max Normalization）</strong><br>
最值归一化是将数据映射到[0,1]范围内，公式如下：</p>
<p>X_normalized = (X - Xmin) / (Xmax - Xmin)</p>
<p>其中，Xmin和Xmax分别是数据集中的最小值和最大值。</p>
<p><strong>均值方差归一化（Standardization）</strong><br>
均值方差归一化是将数据转换为均值为0，方差为1的数据，公式如下：</p>
<p>X_normalized = (X - Xmean) / Xstd</p>
<p>其中，Xmean和Xstd分别是数据集的均值和标准差。</p>
<p><strong>小数定标归一化</strong><br>
小数定标归一化是将数据的小数点移动到某一位置，使得数据范围在一定程度上缩小。例如，将数据的小数点向左移动2位，可以将数据的范围缩小100倍。</p>
<p><strong>对数归一化</strong><br>
对数归一化是将数据取对数后进行归一化处理。这种方法通常用于处理数据的取值范围差异很大的情况。</p>
<p>数据归一化的目的是使得数据的分布更加规律<br>
归一化处理可以使不同的数据具有相似的分布和取值范围，这在许多机器学习算法中是很有用的。例如，在使用梯度下降算法进行模型训练时，如果数据的范围差异很大，模型可能会在局部最小值附近来回震荡，导致收敛速度减慢。归一化处理可以使梯度下降算法更快地收敛，并且可以使模型的泛化能力更强。</p>
<p>但是，也要注意，如果数据已经具有相似的分布和取值范围，则不需要进行归一化处理。另外，在使用归一化处理后的数据进行模型训练后，在使用模型进行预测时，还需要将输入数据进行相应的反归一化处理，才能得到正确的预测结果。</p>
<p><strong>使用pytorch进行归一化操作</strong></p>
<pre><code class="language-python">normalize = transforms.Normalize((mean,), (std,))
</code></pre>
<p>其中 mean 为均值，std为方差，想进行归一化操作需要先计算出数据的均值和方差。</p>
<p>例如对MNIST数据集的处理：</p>
<pre><code class="language-python"># 定义图像处理方法
tranform = transforms.Compose([
    transforms.ToTensor(),  # 将图片转换成Tensor
    transforms.Normalize((0.1307,), (0.3081,))  # 进行数据归一化处理
])
</code></pre>
<p>其中 0.1307 和 0.3081 分别是MNIST的均值和方差，可以自己计算，也可以网上查<br>
等同于 X_normalized = (X - 0.1307) / 0.3081，转化为均值为0，方差为1的数据。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[【线性代数】矩阵的特征值和特征向量]]></title>
        <id>https://jeromezjl.github.io/post/xian-xing-dai-shu-ju-zhen-de-te-zheng-zhi-he-te-zheng-xiang-liang/</id>
        <link href="https://jeromezjl.github.io/post/xian-xing-dai-shu-ju-zhen-de-te-zheng-zhi-he-te-zheng-xiang-liang/">
        </link>
        <updated>2022-12-05T10:23:51.000Z</updated>
        <content type="html"><![CDATA[<p>所谓凡事见微知著，由简推繁，要理解多维矩阵的特征值和特征向量的意思，我们不如从二维推起。<br>
矩阵，又称为矩阵空间，矩阵包含空间信息。1x2 的矩阵，是二维空间上的矩阵</p>
<p>若非零向量 x 满足 Ax = λx，则 λ 为特征值，x 为特征向量。<br>
特征向量在经过矩阵 A 的变换后，不改变，只需要乘上一个常数 λ</p>
<p>对于矩阵特征值和特征向量公式的理解：一个矩阵点乘一个向量就相当于对该向量进行旋转或者拉伸等一系列线性变换。在我们求解矩阵的特征值和特征向量的时候就是要求解矩阵 A，A能够使得这些特征向量只发生伸缩变换，而变换的效果等价于特征向量与某个常量 λ 相乘。<br>
在二维空间理解一下这个式子的含义。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SQL 复习]]></title>
        <id>https://jeromezjl.github.io/post/sql-qi-zhong-fu-xi/</id>
        <link href="https://jeromezjl.github.io/post/sql-qi-zhong-fu-xi/">
        </link>
        <updated>2022-10-27T09:14:16.000Z</updated>
        <content type="html"><![CDATA[<p>复习策略：<br>
看网络资料的数据库复习<br>
弄懂知识点，然后做一下里面的题</p>
<p><a href="https://jeromezjl.github.io/post/sql/">SQL笔记</a></p>
<h1 id="第三章-sql">第三章 SQL</h1>
<p>熟练编写 sql 语句，创建、删除语句也要看，数据类型</p>
<h1 id="第四章">第四章</h1>
<p>◼ Join Expressions（连接表达式）<br>
◼ Views 视图<br>
◼ Transactions<br>
◼ Integrity Constraints<br>
◼ SQL Data Types and Schemas<br>
◼ Authorization<br>
◼ Security in SQL</p>
<p>Join 参见 sql 的总结，natural join，inner join 等</p>
<h1 id="第五章-高级-sql">第五章 高级 SQL</h1>
<p>这章期中没考</p>
<p>使用程序设计语言访问数据库<br>
通过两种方式从通用编程语言访问 SQL：<br>
动态 SQL：通过函数或方法<br>
嵌入式 SQL：在编译时全部确定</p>
<p>JDBC 标准定义了 java 和 SQL 的 API</p>
<h1 id="第六章-关系代数">第六章 关系代数</h1>
<p><a href="https://blog.csdn.net/quinnnorris/article/details/70739094">SQL 形式化语言——关系代数</a><br>
重点是如何将sql语句转化为关系代数</p>
<p>选择运算 select operation σ<br>
选择关系instructor中属于物理系的元组：<br>
σ dept_name=“Physics”(instructor)<br>
选择关系 instructor 中 salary&gt;90000 的元组：<br>
σ salary&gt;90000 (instructor)<br>
⋀ (and), ⋁ (or), (not)</p>
<p>投影运算 project operation  ∏<br>
用于选择表中想要的属性<br>
∏ name, salary (instructor)<br>
找出物理系中所有教师的名字：<br>
∏ name (σ dept_name=“Physic’ (instructor))</p>
<p>并运算 union operation ∪<br>
找出开设在 2009 年球季或者 2010 年春季学期，或者二者皆开的课程集合<br>
∏course_id (σ semester=“Fall” Λ year=2009 (section)) ∪<br>
∏course_id (σ semester=“Spring” Λ year=2010 (section))</p>
<p>集合差运算 Set Difference Operation -<br>
找出开设在 2009 年秋季但不开设在 2010 年春季的课程<br>
∏course_id (σ semester=“Fall”Λ year=2009(section))-<br>
∏course_id (σ semester=“Spring”Λ year=2010(section))</p>
<p>笛卡尔积运算 Cartesian-Product Operation x<br>
∏ name (σ dept_name=“Physic’ (instructor x teaches))</p>
<p>更名运算 Rename Operation<br>
选出 instructor 中 salary 最高的：<br>
σ instructor.salary &lt; d.salary (instructor ╳ ρ d(instructor))</p>
<p>select T.salary<br>
from instructor T, instructor S<br>
where T.salary &gt; S.salary</p>
<h1 id="第七章-e-r-图">第七章 E-R 图</h1>
<p>非ER关系中，两个实体关联，每个实体中可能会有对方实体的主码，从而产生冗余<br>
在ER关系中，如果一个实体的属性是另一个实体的主码，则删除该属性。二者的关联会在联系集里表达。（P183）</p>
<p>双线表示一对一关系，实体集唯一对应关系集</p>
<p>大学实体集及属性（P154 输入183）</p>
<p>大学 E-R 图（P159 输入188）</p>
<p>E-R 图图形含义（P172 输入 201）</p>
<p>练习题（P179 输入208）</p>
<p>弱实体集：如果保留某些属性，则在和别的集合关联时，会造成属性的冗余。所以我们选择删掉该属性。但删掉后，当前剩余的属性就不能保证唯一标识元素。这样的实体集称为弱实体集。弱实体集必须与强实体集关联才有意义。</p>
<p>E-R 图中，双线表示有且仅有一个相关的对象<br>
箭头：A → B 表明 每个 A 至多有一个 B</p>
<p>构造关系表时，如果有多个候选键，最好选取数值型（int, float）候选键作为关系表主键，便于提高基于主键的查询速度<br>
—不要选字符串型属性，如varchar、datetime<br>
—e.g. studentname, instructorName</p>
<p>双菱形表示弱实体集的标识性联系集</p>
<p>构建 E-R 图步骤<br>
从关系集入手，分别判断其关联的两个实体集的对应关系。看是采用双线、箭头，还是单线。<br>
判断弱实体集：看关联的两个实体集的属性，是否有相同的，如果有，去掉其中一个的属性，将其变为弱实体集，然后将关系变为双菱形。</p>
<h1 id="第八章-关系数据库的设计">第八章 关系数据库的设计</h1>
<p>◼ Features of Good Relational Design<br>
◼ Atomic Domains and First Normal Form<br>
◼ Decomposition Using Functional Dependencies<br>
◼ Functional Dependency Theory<br>
◼ Algorithms for Functional Dependencies</p>
<p><strong>好的关系设计的特点</strong><br>
取决于E-R图质量<br>
是否有重复，是否可以很好的表示所有信息<br>
删除和更新问题</p>
<p><a href="https://www.cnblogs.com/sky20080101/articles/8445061.html">原子性和第一范式</a></p>
<h1 id="第十章-数据存储和文件结构">第十章 数据存储和文件结构</h1>
<p>◼ File organization (at physical level, §10.5)<br>
◼ Organization of records in files (at logical level, §10.6 ),<br>
i.e. file structures<br>
◼ Data-dictionary Storage (§10.7)<br>
◼ Data Buffer (§10.8)</p>
<p><strong>10.5 文件组织</strong></p>
<h1 id="第十一章-索引">第十一章 索引</h1>
<p>搜索码：用于在文件中查找记录的属性或属性集<br>
索引<br>
什么是<br>
聚集索引 clustering index ，非聚集索引，稠密索引，稀疏索引，</p>
<p>每个索引对数据增删改查是否有影响，</p>
<p>索引创建在搜索码（属性）上面，可提高检索速度<br>
要会说明原因</p>
<p>在除了查询之外，delete update insert 时如何提高效率，搜索码<br>
索引也有空间、时间开销，增加/删除，所以不一定能对增加删除修改起到提高效率的作用<br>
也可能拖慢速度</p>
<p><strong>on which attributes the indices can be further defined to speed up the query? 在哪里添加索引会继续加快查找？</strong><br>
1.找语句中频繁使用的元素<br>
2.找where后面的元素<br>
3.找join on后面的元素<br>
例：<br>
select branch_city, sum(amount)<br>
from branch inner join loan on branch_name<br>
where assets&gt;1000<br>
group by branch_city<br>
1.branch_city加索引<br>
2.assets加索引<br>
3.branch_name加索引</p>
<p><strong>Give a SQL statement to define a composite index on combined search key(branch_name, amount) on the table loan.  创建索引</strong><br>
语法：<br>
create index branch_name_amount_index on loan(branch_name, amount);<br>
create index 索引名 on 表名（属性名）</p>
<p><strong>Can this index be efficiently used for the following query, and why? 是否可以提升查询速度？</strong><br>
答题角度：看被索引的对象在什么关键词后面。<br>
比如在where后/经常被查询使用</p>
<h1 id="第十二章-查询处理">第十二章 查询处理</h1>
<p>熟悉关系代数的表示<br>
<a href="https://blog.csdn.net/bianyamei/article/details/89491358">启发式查询树优化实例</a><br>
<a href="https://www.bilibili.com/video/BV1Fp4y1a7Cs/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">B站讲解 数据库语法树优化</a></p>
<p>步骤<br>
1、写出关系代数表达式<br>
2、画出查询树<br>
3、选择关系下移<br>
4、投影下移，注意投影要在选择关系之上；每个自然连接前面必须有投影</p>
<h1 id="函数依赖">函数依赖</h1>
<p><a href="https://blog.csdn.net/m0_46670811/article/details/109526906">求属性集闭包(AB+)</a><br>
<a href="https://blog.csdn.net/Game_Zmh/article/details/88058069">求属性集闭包</a></p>
<p><strong>List all the candidate keys of R. 求候选键</strong><br>
1.<a href="https://blog.csdn.net/YYbLQQ/article/details/124508824">即只出现在F箭头左边的一定是候选码</a><br>
上例中，A和C只出现在箭头左边，我们用闭包计算可以得到U，则（AC）是该关系的候选码<br>
注：除了候选键，其余都是非主属性<br>
2.可以推出所有的字母的是候选键</p>
<p><strong>What is the highest normal form of R 关系R的最高范式是什么（判断是第几范式）</strong><br>
<a href="https://blog.csdn.net/m0_37345402/article/details/106163096">如何判断范式（1NF、2NF、3NF、BCNF）</a><br>
1.求候选码。候选键可以是连着的几个字母，而主属性是分开的独立字母；除了主属性之外的字母都是非主属性<br>
2.如果F中，存在左部没有候选码的关系，则有非平凡FD，则不是BCNF<br>
3.如果上述关系中，右部都是主属性，则为3NF<br>
4.如果任何主属性都不能推出非主属性，则为2NF<br>
5.否则为1NF</p>
<p><strong>lossless-join decomposition 判断无损分解</strong><br>
<a href="https://blog.csdn.net/qq_41338249/article/details/112724127">无损分解讲解</a></p>
<p><strong>Compute the canonical cover Fc 求最小函数依赖</strong><br>
<a href="https://www.zhihu.com/question/21235096">知乎讲解</a><br>
1.将右部都化为单一字母<br>
2.去掉左边多余属性，具体方法：只需关注非单属性，求左侧的闭包，但不要加上左边推出的字母，比如 DG-&gt;C：判断(DG)+ = DG，不要再把C加上再推了。观察得到的闭包，如果闭包含有右侧推出的字母，则删除；否则保留。DG不含C，则保留。<br>
3.剩余的单个关系，判断是否存在形如 D-&gt;C, C-&gt;A, D-&gt;A 这样的，删除 D-&gt;A 即可<br>
综上可得出最小函数依赖</p>
<p><strong>Give a lossless-join and dependency-preserving decomposition of R into 3NF</strong><br>
在最小函数依赖上进行构建</p>
<h1 id="事务">事务</h1>
<p>concurrent transactions：并发事务<br>
<a href="https://www.youtube.com/watch?v=U3SHusK80q0">画优先图 precedence graph 并 判断是否可串行化 serializable：油管清晰讲解</a><br>
关注三对关系：R-&gt;W，W-&gt;R，W-&gt;W</p>
<p><strong>是否是可恢复调度？ Is S a recoverable schedule, and why?</strong><br>
对于可恢复调度，如果一个调度从另外一个调度的结果中读取数据，那么它必须在另外以后调度的commit之后commit。<br>
不是可恢复调度的例子：<br>
T2 读取 student 表中 stuID=10 的元组时，该元组内容已由 T1 修改过，但 T2 提交操作<br>
commit 早于 T1 提交 commit。一旦 T1 在 T2 的 commit 操作之后回滚其 update student 操作，将 stuName 回滚为旧值，则 T2 的 select 操作无法随着回滚，T2 读取的仍然是 stuName 修改后的值。</p>
<p><strong>是否是无级联调度？Is S a cascadeless schedule, and why?</strong><br>
如果事务要对某个值执行读操作，则必须等到执行写入该值的事务 commit。</p>
<p><strong>是否满足二阶段锁？ obey the two-phase locking protocol？</strong><br>
在同一个事务中，前半部分全是lock，后面全是unlock，则满足，否则不满足<br>
注意是在同一个事务中，如在T1这一列中看。</p>
<p><strong>是否满足严格二阶段锁？ Strict two phase locking protocol</strong><br>
共享锁（share）：lock_s；排他锁（exclusive）：lock_x<br>
S2PLP 可以在 lock_s上锁之后随时释放它，但只能在 commit 的时候释放 lock_x<br>
严锁在commit的时候同时释放所有锁。其主要区别简单来说，就是：2PL能随时释放锁，S2PL只能在事务结束后释放锁。但注意，随时释放不是指</p>
<p><strong>是否满足严格二阶段锁？Rigorous two phase locking protocol</strong><br>
R2PLP 只能在 commit 的时候释放所有锁， lock_s 和 lock_x</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ML/DL 中常见的英文]]></title>
        <id>https://jeromezjl.github.io/post/mldl-zhong-chang-jian-de-ying-wen/</id>
        <link href="https://jeromezjl.github.io/post/mldl-zhong-chang-jian-de-ying-wen/">
        </link>
        <updated>2022-10-25T14:06:00.000Z</updated>
        <content type="html"><![CDATA[<p>parameters / params：参数<br>
epochs：总训练次数，使用全部数据训练一次是一个epochs<br>
learning rate：学习率<br>
optimize：优化<br>
regularization：正则化<br>
normalize：归一化、标准化<br>
Regularization： 正则化</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[以梯度下降（Gradient Descent）展开的优化器总结]]></title>
        <id>https://jeromezjl.github.io/post/ti-du-xia-jiang-gradient-descent/</id>
        <link href="https://jeromezjl.github.io/post/ti-du-xia-jiang-gradient-descent/">
        </link>
        <updated>2022-10-25T12:57:09.000Z</updated>
        <content type="html"><![CDATA[<p>本篇关键词：损失函数/代价函数/误差函数、梯度下降、学习率、Momentum（动量）</p>
<p><a href="https://zhuanlan.zhihu.com/p/68468520">梯度下降</a><br>
<a href="https://zhuanlan.zhihu.com/p/357963858">随机梯度下降</a></p>
<p>大多数机器学习模型都会有一个损失函数。比如常见的 均方误差 （Mean Squared Error —— MSE）损失函数，其输出值为 模型的输出值和实际值的偏差。<br>
损失函数的输出值越小，模型精度越高。我们用梯度下降的方法最小化损失函数。</p>
<p>梯度下降（Gradient Descent）：使用所有样本进行梯度下降<br>
小批量样本梯度下降（Mini Batch GD）：使用小批量样本进行梯度下降<br>
随机梯度下降（Stochastic GD）：使用一个样本进行梯度下降</p>
<p><a href="https://www.zhihu.com/question/395685065/answer/2535950728">怎么通俗易懂的理解SGD中Momentum的含义？</a></p>
<p>SGD 的改进形式 —— Adam</p>
<p><a href="https://www.zhihu.com/question/42115548/answer/1636798770">SGD有多种改进的形式(RMSprop,Adadelta等),为什么大多数论文中仍然用SGD?</a></p>
<p><a href="https://blog.csdn.net/qq_42109740/article/details/105401197">深度学习各类优化器详解（动量、NAG、adam、Adagrad、adadelta、RMSprop、adaMax、Nadam、AMSGrad）</a></p>
<p><a href="https://blog.csdn.net/qq_36589234/article/details/89330342">PyTorch学习之 torch.optim 的6种优化器及优化算法介绍</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[KPL 语言]]></title>
        <id>https://jeromezjl.github.io/post/kpl-yu-yan/</id>
        <link href="https://jeromezjl.github.io/post/kpl-yu-yan/">
        </link>
        <updated>2022-10-16T09:38:19.000Z</updated>
        <content type="html"><![CDATA[<h1 id="编译过程">编译过程</h1>
<p>编译 package Hello 生成 Hello.s</p>
<pre><code class="language-C">kpl Hello
</code></pre>
<p>编译 Hello.s 生成 Hello.o</p>
<pre><code class="language-C">asm Hello.s
</code></pre>
<p>combine all of the &quot;.o&quot; object files into an executable file</p>
<pre><code class="language-c">lddd System.o Hello.o Runtime.o -o Hello
</code></pre>
<p>with the &quot;-o&quot; option , the new file will be named &quot;Hello&quot; or will be &quot;a.out&quot;</p>
<p>To run the package &quot;Hello&quot;</p>
<pre><code class="language-C">blitz -g Hello
</code></pre>
<p>&quot;-g&quot; means run it directly</p>
<p>After execution completes,enter &quot;q&quot; to quit</p>
<h1 id="the-header-and-code-files">The Header and Code Files</h1>
<p>A program is made of several packages and each package is described by a header file and a code file</p>
<p>The header file is the specification for the package. It provides the external interface to that package,giving all information other packages will need about what is in the package. In the Hello-World example, the file “Hello.h” specifies the package will contain a function called “main” and tells what parameters this function takes and returns. (The main function takes no parameters and returns no results.)</p>
<p>The code file contains the implementation details for the package. All executable code appears in the code file. In the Hello-World example, the “Hello.c” file contains the actual code for the main function</p>
<h1 id="编译过程-2">编译过程</h1>
<p>直接输入 make，在文件夹内会根据 makefile 中的规则编译所有文件<br>
再次输入 blitz -g os 运行所有代码</p>
<p>https://github.com/ayushishri/OS-Blitz-Labs</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MNIST 手写数字识别]]></title>
        <id>https://jeromezjl.github.io/post/mnist-shou-xie-shu-zi-shi-bie/</id>
        <link href="https://jeromezjl.github.io/post/mnist-shou-xie-shu-zi-shi-bie/">
        </link>
        <updated>2022-10-14T07:23:30.000Z</updated>
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/m0_58092763/article/details/125631991?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-125631991-blog-112980305.t5_landing_title_tags&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-125631991-blog-112980305.t5_landing_title_tags&amp;utm_relevant_index=2">手把手实战PyTorch手写数据集MNIST识别项目全流程</a></p>
<p><a href="https://blog.csdn.net/wqy1837154675/article/details/108003698">pytorch读取MNIST数据集并显示</a></p>
<h1 id="理论">理论</h1>
<p><strong>数据读取和预处理</strong><br>
CUDA：显卡驱动，有了CUDA显卡才能进行复杂运算<br>
<a href="https://blog.csdn.net/wuzhongqiang/article/details/105499476">数据读取 DataLoader 和 图像预处理 transforms 详解（这篇值得反复阅读）</a><br>
<a href="https://www.jianshu.com/p/22c50ded4cf7">深度学习 | 三个概念：Epoch, Batch, Iteration</a><br>
<a href="https://blog.csdn.net/weixin_43135178/article/details/115133115">使用 transforms.Compose() 将图像变换方法整合在一起</a><br>
<a href="https://blog.csdn.net/qq_37555071/article/details/107532319">torchvision.transforms 对有限的图片数据进行各种变换，如缩小或者放大图片的大小、对图片进行水平或者垂直翻转等，这些都是数据增强的方法。</a><br>
<a href="https://www.pudn.com/news/6355f71da4b7e43a5ea8bc09.html#%E3%80%90%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE%E3%80%91">使用sklearn实现对数据集划分，从而进行交叉验证</a></p>
<p><strong>可视化工具包</strong><br>
<a href="https://blog.csdn.net/zkp_987/article/details/81748098">以进度条形式可视化迭代器运行的包：tqdm</a></p>
<p><a href="https://www.bilibili.com/video/BV1K64y1Q7wu?p=3&amp;vd_source=3d9ada7d42c971c0c3f04a22270daf33">李沐 Softmax 函数</a></p>
<p>对 softmax 回归的感性理解：<br>
线性回归模型多输入，单输出，而 softmax 多输入，多输出。输出结果为输入的向量所属类别，由于类别有多个，所以多输出。输出概率最大的为所属类别。但由于直接输出的概率值 总和不一定为1，且可能有负值，所以对多个输出结果进行 softmax 方法的计算，从而使得输出结果为 总和为1，且均为正值的概率值。</p>
<h1 id="实现">实现</h1>
<pre><code class="language-python"># 导入库
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from sklearn.model_selection._split import KFold
</code></pre>
<pre><code class="language-python"># 定义超参数
BATCH_SIZE = 128  # 每批处理的数据量
DEVICE = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)  # 用CPU还是GPU训练
EPOCHS = 10  # 定义总训练次数
k_split_value = 5  # 定义 5 折交叉验证
</code></pre>
<pre><code class="language-python"># 定义图像处理方法
tranform = transforms.Compose([
    transforms.ToTensor(),  # 将图片转换成Tensor
    transforms.Normalize((0.1307,), (0.3081,))  # 进行数据归一化处理
])
</code></pre>
<pre><code class="language-python"># 下载、加载数据集
from torch.utils.data import DataLoader

train_data = datasets.MNIST(root=&quot;./MNIST&quot;,
                            train=True,
                            transform=tranform,
                            download=False)  # 注，如果已经下载了一遍，也需要用该命令设置data的路径

test_data = datasets.MNIST(root=&quot;./MNIST&quot;,
                           train=False,
                           transform=tranform,
                           download=False)

# 将测试集和训练集合并，以便后续对数据集进行分割
dataFold = torch.utils.data.ConcatDataset([train_data, test_data])
</code></pre>
<pre><code class="language-python"># 构建网络模型，使用 AlexNet
class AlexNet(nn.Module):
    def __init__(self):
        super(AlexNet, self).__init__()

        # 由于MNIST为28x28， 而最初AlexNet的输入图片是227x227的。所以网络层数和参数需要调节
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # AlexCONV1(3,96, k=11,s=4,p=0)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # AlexPool1(k=3, s=2)
        self.relu1 = nn.ReLU()

        # self.conv2 = nn.Conv2d(96, 256, kernel_size=5,stride=1,padding=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # AlexCONV2(96, 256,k=5,s=1,p=2)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # AlexPool2(k=3,s=2)
        self.relu2 = nn.ReLU()

        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # AlexCONV3(256,384,k=3,s=1,p=1)
        # self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)
        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)  # AlexCONV4(384, 384, k=3,s=1,p=1)
        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)  # AlexCONV5(384, 256, k=3, s=1,p=1)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # AlexPool3(k=3,s=2)
        self.relu3 = nn.ReLU()

        self.fc6 = nn.Linear(256 * 3 * 3, 1024)  # AlexFC6(256*6*6, 4096)
        self.fc7 = nn.Linear(1024, 512)  # AlexFC6(4096,4096)
        self.fc8 = nn.Linear(512, 10)  # AlexFC6(4096,1000)
        self.dropout = nn.Dropout(p=0.5)

    def forward(self, x):
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.relu1(x)
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.relu2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.conv5(x)
        x = self.pool3(x)
        x = self.relu3(x)
        x = x.view(-1, 256 * 3 * 3)  # Alex: x = x.view(-1, 256*6*6)
        x = self.fc6(x)
        x = self.dropout(x)
        x = F.relu(x)
        x = self.fc7(x)
        x = self.dropout(x)
        x = F.relu(x)
        x = self.fc8(x)
        return x
</code></pre>
<pre><code class="language-python"># 定义优化器
model = AlexNet().to(DEVICE)
optimizer = optim.Adam(model.parameters())  # 使用 Adam 优化器
</code></pre>
<pre><code class="language-python"># 定义训练方法
def train_model(model, device, train_loader, optimizer, epoch):
    model.train()  # PyTorch 提供的训练方法
    for batch_index, (data, label) in enumerate(train_loader):
        # 部署到DEVICE
        data, label = data.to(device), label.to(device)
        # 梯度初始化为0
        optimizer.zero_grad()
        # 训练后的结果
        output = model(data)
        # 计算损失（针对多分类任务交叉熵，二分类用sigmoid）
        loss = F.cross_entropy(output, label)
        # 找到最大概率的下标
        pred = output.argmax(dim=1)
        # 反向传播Backpropagation
        loss.backward()
        # 参数的优化
        optimizer.step()
        if batch_index % 3000 == 0:
            print(&quot;Train Epoch : {} \t Loss : {:.6f}&quot;.format(epoch, loss.item()))
</code></pre>
<pre><code class="language-python"># 定义测试方法
def test_model(model, device, test_loader):
    # 模型验证
    model.eval()
    # 统计正确率
    correct = 0.0
    # 测试损失
    test_loss = 0.0
    with torch.no_grad():  # 不计算梯度，不反向传播
        for data, label in test_loader:
            data, label = data.to(device), label.to(device)
            # 测试数据
            output = model(data)
            # 计算测试损失
            test_loss += F.cross_entropy(output, label).item()
            # 找到概率值最大的下标
            pred = output.argmax(dim=1)
            # 累计正确率
            correct += pred.eq(label.view_as(pred)).sum().item()
        test_loss /= len(test_loader.dataset)
        print(&quot;Test —— Average loss : {:.4f}, Accuracy : {:.3f}\n&quot;.format(test_loss,
                                                                          100.0 * correct / len(test_loader.dataset)))
</code></pre>
<pre><code class="language-python"># 对数据进行 K 折交叉划分并且调用前面的方法训练模型
def KFold_and_Train(k_split_value):
    counter = 1  # 自定义一个交叉验证计数器
    kf = KFold(n_splits=k_split_value, shuffle=True, random_state=0)  # 定义K折交叉验证的方法
    for train_index, test_index in kf.split(dataFold):  # 使用kf方法将dataFold分成测试集和验证集
        print(f&quot;第{counter}次交叉验证&quot;)
        counter += 1  # 计数器自增1

        # get train, val
        train_fold = torch.utils.data.dataset.Subset(dataFold, train_index)
        test_fold = torch.utils.data.dataset.Subset(dataFold, test_index)

        # package type of DataLoader
        train_loader = torch.utils.data.DataLoader(dataset=train_fold, batch_size=BATCH_SIZE, shuffle=True)
        test_loader = torch.utils.data.DataLoader(dataset=test_fold, batch_size=BATCH_SIZE, shuffle=True)

        # training and test
        for epoch in range(EPOCHS):
            train_model(model, DEVICE, train_loader, optimizer, epoch)
            test_model(model, DEVICE, test_loader)
</code></pre>
<pre><code class="language-python"># 调用函数
KFold_and_Train(k_split_value)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Novelai 使用]]></title>
        <id>https://jeromezjl.github.io/post/novelai-shi-yong/</id>
        <link href="https://jeromezjl.github.io/post/novelai-shi-yong/">
        </link>
        <updated>2022-10-13T06:07:39.000Z</updated>
        <content type="html"><![CDATA[<p><a href="https://www.bilibili.com/video/BV1EV4y1L7dX/?vd_source=3d9ada7d42c971c0c3f04a22270daf33">教程</a></p>
<p>启动：浏览器输入域名：127.0.0.1:6969</p>
<p><a href="https://zhuanlan.zhihu.com/p/572865961">参数教程</a></p>
]]></content>
    </entry>
</feed>